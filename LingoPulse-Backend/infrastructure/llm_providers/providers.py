"""
Infrastructure Layer - LLM Providers
åŸºç¡€è®¾æ–½å±‚ - LLMæä¾›å•†æ¥å£
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import httpx
import json


@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®"""
    content: str
    confidence: float
    usage: Dict[str, int]
    model: str
    finish_reason: str


class LLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
    
    @abstractmethod
    async def analyze_sentiment(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        pass
    
    @abstractmethod
    async def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        pass
    
    @abstractmethod
    async def calculate_complexity(self, text: str) -> float:
        """è®¡ç®—è¯­è¨€å¤æ‚åº¦"""
        pass
# class WenxinProvider(LLMProvider):
#     """æ–‡å¿ƒä¸€è¨€æä¾›å•†"""
    
#     def __init__(self, api_key: str, secret_key: str):
#         super().__init__(api_key, "https://aip.baidubce.com/rpc/2.0")
#         self.secret_key = secret_key
#         self.access_token = None
#         self.client = httpx.AsyncClient()
    
#     async def _get_access_token(self) -> str:
#         """è·å–è®¿é—®ä»¤ç‰Œ"""
#         if self.access_token:
#             return self.access_token
        
#         try:
#             response = await self.client.post(
#                 f"https://aip.baidubce.com/oauth/2.0/token",
#                 data={
#                     "grant_type": "client_credentials",
#                     "client_id": self.api_key,
#                     "client_secret": self.secret_key
#                 }
#             )
            
#             if response.status_code == 200:
#                 result = response.json()
#                 self.access_token = result["access_token"]
#                 return self.access_token
#             else:
#                 raise Exception("Failed to get access token")
                
#         except Exception:
#             raise Exception("Failed to authenticate with Wenxin")
    
#     async def analyze_sentiment(self, text: str) -> float:
#         """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
#         try:
#             access_token = await self._get_access_token()
            
#             # ä½¿ç”¨æ–‡å¿ƒçš„æƒ…æ„Ÿåˆ†æAPI
#             response = await self.client.post(
#                 f"{self.base_url}/nlp/v1/sentiment_classify?access_token={access_token}",
#                 json={"text": text}
#             )
            
#             if response.status_code == 200:
#                 result = response.json()
#                 # æ–‡å¿ƒè¿”å›çš„æ˜¯åˆ†ç±»ç»“æœï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°å€¼
#                 items = result.get("items", [])
#                 if items:
#                     sentiment_label = items[0].get("sentiment", 0)
#                     # 0:è´Ÿé¢, 1:ä¸­æ€§, 2:æ­£é¢
#                     if sentiment_label == 2:
#                         return 0.7
#                     elif sentiment_label == 1:
#                         return 0.0
#                     else:
#                         return -0.7
#                 return 0.0
#             else:
#                 return 0.0
                
#         except Exception:
#             return 0.0
    
#     async def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
#         """æå–å…³é”®è¯ - ä½¿ç”¨æ–‡å¿ƒçš„æ–‡æœ¬ç›¸ä¼¼åº¦åŠŸèƒ½"""
#         # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„NLPåŠŸèƒ½
#         try:
#             # æ¨¡æ‹Ÿå…³é”®è¯æå–
#             words = text.split()
#             # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
#             keywords = [word for word in words if len(word) > 1 and word not in {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ'}]
#             return keywords[:max_keywords]
#         except Exception:
#             return []
    
#     async def calculate_complexity(self, text: str) -> float:
#         """è®¡ç®—è¯­è¨€å¤æ‚åº¦"""
#         try:
#             # ç®€åŒ–å®ç°ï¼šåŸºäºå¥å­é•¿åº¦å’Œè¯æ±‡å¤æ‚åº¦
#             sentences = text.split('ã€‚') + text.split('.') + text.split('!') + text.split('?')
#             if sentences:
#                 avg_length = sum(len(s.split()) for s in sentences) / len(sentences)
#                 return min(avg_length / 20, 1.0)
#             return 0.5
#         except Exception:
#             return 0.5
    
#     async def close(self):
#         """å…³é—­å®¢æˆ·ç«¯"""
#         await self.client.aclose()


class LocalModelProvider(LLMProvider):
    """æœ¬åœ°æ¨¡å‹æä¾›å•†ï¼ˆå¢å¼ºå®ç°ï¼‰"""
    
    def __init__(self, model_path: str):
        super().__init__("", "")
        self.model_path = model_path
        self.client = httpx.AsyncClient()
        print("ğŸ¤– åˆå§‹åŒ–æœ¬åœ°AIæ¨¡å‹æä¾›å•†ï¼ˆå¢å¼ºç‰ˆï¼‰")
    
    async def analyze_sentiment(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ - å¢å¼ºæœ¬åœ°å®ç°"""
        print(f"ğŸ” æœ¬åœ°AIåˆ†ææƒ…æ„Ÿ: {text[:30]}...")
        
        # æ›´ä¸°å¯Œçš„æƒ…æ„Ÿè¯æ±‡åº“ï¼ŒæŒ‰å¼ºåº¦åˆ†çº§
        very_positive_words = {'å¤ªå¥½äº†', 'å®Œç¾', 'æ°å‡º', 'å‡ºè‰²', 'æ„ŸåŠ¨', 'æ¿€åŠ¨', 'å´‡æ‹œ', 'æ•¬ä½©', 'çˆ±æ­»', 'è¶…çº§æ£’', 'æƒŠè‰³', 'éœ‡æ’¼'}
        positive_words = {'å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'é«˜å…´', 'å¿«ä¹', 'æ»¡æ„', 'èµ', 'ç²¾å½©', 'å¹¸ç¦', 'ç¾å¥½', 'æ„‰å¿«', 'å¼€å¿ƒ', 'æ„Ÿè°¢', 'æ„Ÿæ©'}
        neutral_words = {'è¿˜è¡Œ', 'ä¸€èˆ¬', 'æ™®é€š', 'å¹³å¸¸', 'ä¸­è§„ä¸­çŸ©', 'å¯ä»¥', 'å‡‘åˆ'}
        negative_words = {'å', 'å·®', 'è®¨åŒ', 'æ¨', 'éš¾è¿‡', 'å¤±æœ›', 'æ²®ä¸§', 'ç—›è‹¦', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ç„¦è™‘', 'æ‹…å¿ƒ', 'å®³æ€•', 'ç»æœ›', 'æ— èŠ', 'çƒ¦èº'}
        very_negative_words = {'ç³Ÿç³•', 'æ¶åŠ£', 'å¯æ€•', 'å´©æºƒ', 'ç»æœ›', 'æ¶å¿ƒ', 'åæ„Ÿ', 'æ„¤æ…¨', 'ä»‡æ¨', 'ææ€–'}
        
        # æ›´ç»†è‡´çš„æƒ…æ„Ÿåˆ†æ
        words = text.split()
        
        # åŸºç¡€ç»Ÿè®¡
        very_positive_count = sum(1 for word in words if word in very_positive_words)
        positive_count = sum(1 for word in words if word in positive_words)
        neutral_count = sum(1 for word in words if word in neutral_words)
        negative_count = sum(1 for word in words if word in negative_words)
        very_negative_count = sum(1 for word in words if word in very_negative_words)
        
        # æƒ…æ„Ÿå¼ºåº¦ä¿®é¥°è¯
        intensity_modifiers = {
            'éå¸¸': 1.3, 'ç‰¹åˆ«': 1.2, 'æå…¶': 1.5, 'è¶…çº§': 1.4, 'ååˆ†': 1.2, 'ç›¸å½“': 1.1, 
            'æ ¼å¤–': 1.2, 'è¶…': 1.4, 'å¤ª': 1.2, 'ç‰¹åˆ«': 1.2, 'çœŸçš„': 1.1, 'ç¡®å®': 1.1,
            'æŒº': 1.0, 'è›®': 1.0, 'è›®å¥½': 1.1, 'æŒºæ£’': 1.1
        }
        
        # åº”ç”¨å¼ºåº¦ä¿®é¥°
        for word in words:
            if word in intensity_modifiers:
                modifier = intensity_modifiers[word]
                # å‘å‰æŸ¥æ‰¾ä¸€ä¸ªè¯çœ‹æ˜¯å¦åŒ¹é…æƒ…æ„Ÿè¯
                for i in range(len(words)-1):
                    if i < len(words)-1 and words[i] == word:
                        next_word = words[i+1]
                        if next_word in positive_words or next_word in very_positive_words:
                            positive_count *= modifier
                        elif next_word in negative_words or next_word in very_negative_words:
                            negative_count *= modifier
        
        # æ–‡æœ¬é•¿åº¦å’Œå¤æ‚åº¦å½±å“
        text_complexity = min(len(text) / 100, 1.0)
        
        # è®¡ç®—æœ€ç»ˆæƒ…æ„Ÿåˆ†æ•° - ä½¿ç”¨åŠ æƒè®¡ç®—
        positive_score = (positive_count * 1.0 + very_positive_count * 1.5) / max(len(words), 1) * 0.8
        negative_score = (negative_count * 1.0 + very_negative_count * 1.5) / max(len(words), 1) * 0.8
        
        # è€ƒè™‘æ ‡ç‚¹ç¬¦å·
        exclamation_count = text.count('!') + text.count('ï¼')
        question_count = text.count('?') + text.count('ï¼Ÿ')
        emotion_punctuation = (exclamation_count * 0.1) - (question_count * 0.05)
        
        # ç»¼åˆè®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        sentiment = positive_score - negative_score + emotion_punctuation
        sentiment = max(-1.0, min(1.0, sentiment))
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æƒ…æ„Ÿè¯ï¼ŒåŸºäºæ–‡æœ¬ç‰¹å¾ç»™å‡ºå¤§è‡´åˆ¤æ–­
        if very_positive_count + positive_count + neutral_count + negative_count + very_negative_count == 0:
            # åŸºäºæ–‡æœ¬é•¿åº¦å’Œç‰¹å¾ç»™å‡ºåŸºç¡€æƒ…æ„Ÿåˆ¤æ–­
            if '?' in text or 'ï¼Ÿ' in text:
                sentiment = 0.1  # é—®é¢˜é€šå¸¸è¡¨ç¤ºä¸­æ€§åˆ°è½»å¾®ç§¯æ
            elif len(text) > 50:
                sentiment = 0.05  # é•¿æ–‡æœ¬é€šå¸¸æ¯”è¾ƒä¸­æ€§
            else:
                sentiment = 0.0
        
        print(f"ğŸ“Š æƒ…æ„Ÿåˆ†æç»“æœ: {sentiment:.3f} (æç§¯æ:{very_positive_count}, ç§¯æ:{positive_count}, æ¶ˆæ:{negative_count}, ææ¶ˆæ:{very_negative_count})")
        return sentiment
    
    async def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """æå–å…³é”®è¯ - å¢å¼ºæœ¬åœ°å®ç°"""
        print(f"ğŸ” æœ¬åœ°AIæå–å…³é”®è¯: {text[:30]}...")
        
        # åœç”¨è¯åˆ—è¡¨
        stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å› ä¸º', 'æ‰€ä»¥', 
            'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸€ä¸ª', 'ä¸€äº›', 'å¾ˆ', 'éå¸¸', 'ä¹Ÿ', 'éƒ½', 'è¿˜', 'å°±', 
            'å¦‚æœ', 'è™½ç„¶', 'å¯æ˜¯', 'ä¸è¿‡', 'ç„¶è€Œ', 'å› æ­¤', 'æ‰€ä»¥', 'è€Œä¸”', 'æˆ–è€…',
            'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å“ªé‡Œ', 'è°', 'å—', 'å‘¢', 'çš„', 'åœ°', 'å¾—'
        }
        
        # å…³é”®è¯æå–é€»è¾‘
        words = text.split()
        word_freq = {}
        
        # ç»Ÿè®¡è¯é¢‘
        for word in words:
            clean_word = word.strip('ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''()ï¼ˆï¼‰').lower()
            if clean_word and len(clean_word) > 1 and clean_word not in stop_words:
                word_freq[clean_word] = word_freq.get(clean_word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºå¹¶æå–å…³é”®è¯
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        result = [word for word, freq in keywords[:max_keywords]]
        
        print(f"ğŸ“Š å…³é”®è¯æå–ç»“æœ: {result}")
        return result
    
    async def calculate_complexity(self, text: str) -> float:
        """è®¡ç®—è¯­è¨€å¤æ‚åº¦ - å¢å¼ºå®ç°"""
        print(f"ğŸ§  æœ¬åœ°AIè®¡ç®—å¤æ‚åº¦: {text[:30]}...")
        
        try:
            import re
            
            # åˆ†æè¯­è¨€å¤æ‚åº¦æŒ‡æ ‡
            words = text.split()
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            # æŒ‡æ ‡1: å¹³å‡å¥é•¿
            if sentences:
                avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
            else:
                avg_sentence_length = len(words)
            
            # æŒ‡æ ‡2: è¯æ±‡å¤šæ ·æ€§ï¼ˆä¸åŒè¯æ±‡å æ€»è¯æ±‡çš„æ¯”ä¾‹ï¼‰
            unique_words = set(words)
            lexical_diversity = len(unique_words) / max(len(words), 1)
            
            # æŒ‡æ ‡3: æ ‡ç‚¹ç¬¦å·å¤æ‚åº¦
            punctuation_marks = ['ï¼Œ', 'ã€', 'ï¼›', 'ï¼š', 'â€”â€”', 'â€¦', 'â€¦', 'ï¼ˆ', 'ï¼‰', 'ã€Š', 'ã€‹']
            punctuation_count = sum(1 for char in text if char in punctuation_marks)
            punctuation_density = punctuation_count / max(len(text), 1)
            
            # æŒ‡æ ‡4: è¿æ¥è¯å’Œé€»è¾‘è¯ä½¿ç”¨
            logical_words = ['å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¦‚æœ', 'è™½ç„¶', 'å³ä½¿', 'å› æ­¤', 'æ­¤å¤–', 'å¦å¤–', 'è€Œä¸”', 'æˆ–è€…', 'ä»¥åŠ', 'æˆ–è€…è¯´', 'æ›´é‡è¦', 'å€¼å¾—æ³¨æ„çš„æ˜¯']
            logical_count = sum(1 for word in words if word in logical_words)
            logical_density = logical_count / max(len(words), 1)
            
            # ç»¼åˆå¤æ‚åº¦è®¡ç®—
            length_factor = min(avg_sentence_length / 15, 1.0)  # å¥é•¿å› å­
            diversity_factor = lexical_diversity  # è¯æ±‡å¤šæ ·æ€§
            punctuation_factor = min(punctuation_density * 20, 1.0)  # æ ‡ç‚¹å¯†åº¦
            logical_factor = min(logical_density * 10, 1.0)  # é€»è¾‘è¯å¯†åº¦
            
            # åŠ æƒç»¼åˆ
            complexity = (
                0.4 * length_factor +
                0.3 * diversity_factor +
                0.2 * punctuation_factor +
                0.1 * logical_factor
            )
            
            final_complexity = max(0.0, min(1.0, complexity))
            print(f"ğŸ¯ å¤æ‚åº¦åˆ†æ: {final_complexity:.3f} (å¥é•¿:{avg_sentence_length:.1f}, å¤šæ ·æ€§:{lexical_diversity:.2f})")
            
            return final_complexity
            
        except Exception as e:
            print(f"âŒ å¤æ‚åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


class BaiduAistudioProvider(LLMProvider):
    """ç™¾åº¦AI Studioæ¨ç†æ¨¡å‹æä¾›å•†"""
    
    def __init__(self, access_token: str):
        super().__init__(access_token, "https://aistudio.baidu.com/llm/lmapi/v3")
        self.client = httpx.AsyncClient(timeout=120.0)  # è®¾ç½®120ç§’è¶…æ—¶
        self.model = "ernie-3.5-8k"  # ä¿®å¤æ¨¡å‹åç§°
        print("ğŸ¤– åˆå§‹åŒ–ç™¾åº¦AI Studioæ¨ç†æ¨¡å‹æä¾›å•†")
    
    async def analyze_sentiment(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        prompt = f"""
        åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œè¿”å›-1åˆ°1ä¹‹é—´çš„æ•°å€¼ï¼š
        -1è¡¨ç¤ºå¼ºçƒˆè´Ÿé¢ï¼Œ0è¡¨ç¤ºä¸­æ€§ï¼Œ1è¡¨ç¤ºå¼ºçƒˆæ­£é¢
        æ–‡æœ¬ï¼š{text}
        """
        
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 50,
                    "temperature": 0.3  # å¢åŠ æ¸©åº¦ï¼Œè®©ç»“æœæ›´æœ‰å·®å¼‚æ€§
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                print(f"ğŸ” ç™¾åº¦APIæƒ…æ„Ÿåˆ†æåŸå§‹å“åº”: {content}")
                
                # æå–æ•°å€¼
                import re
                numbers = re.findall(r'[-+]?\d*\.?\d+', content)
                if numbers:
                    try:
                        sentiment_score = float(numbers[0])
                        # ç¡®ä¿åˆ†æ•°åœ¨[-1, 1]èŒƒå›´å†…
                        sentiment_score = max(-1.0, min(1.0, sentiment_score))
                        print(f"ğŸ“Š æå–çš„æƒ…æ„Ÿåˆ†æ•°: {sentiment_score}")
                        return sentiment_score
                    except ValueError:
                        print(f"âŒ æ— æ³•è½¬æ¢æƒ…æ„Ÿåˆ†æ•°: {numbers[0]}")
                        return 0.0
                else:
                    print(f"âŒ æœªæ‰¾åˆ°æ•°å­—ï¼ŒåŸå§‹å†…å®¹: {content}")
                    return 0.0
            else:
                print(f"ç™¾åº¦AI Studio APIé”™è¯¯: {response.status_code}")
                return 0.0
                
        except Exception as e:
            print(f"ç™¾åº¦AI Studioæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return 0.0
    
    async def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        prompt = f"""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{max_keywords}ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œä»¥é€—å·åˆ†éš”ï¼š
        æ–‡æœ¬ï¼š{text}
        """
        
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 300,  # å¤§å¹…å¢åŠ å…³é”®è¯æå–çš„tokené™åˆ¶
                    "temperature": 0.7  # æé«˜æ¸©åº¦ï¼Œè®©å…³é”®è¯æå–æ›´çµæ´»
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                print(f"ğŸ” ç™¾åº¦APIå…³é”®è¯æå–åŸå§‹å“åº”: {content}")
                
                # å¤„ç†å¯èƒ½çš„å…³é”®è¯å“åº”æ ¼å¼
                import re
                keywords = []
                
                # å°è¯•ä»¥é€—å·åˆ†éš”
                if ',' in content:
                    potential_keywords = [kw.strip() for kw in content.split(",")]
                    for kw in potential_keywords:
                        if kw and len(kw) <= 10:  # è¿‡æ»¤æ‰å¤ªé•¿çš„æè¿°
                            keywords.append(kw)
                
                # å¦‚æœæ²¡æœ‰é€—å·åˆ†éš”ï¼Œå°è¯•æå–å¯èƒ½çš„å…³é”®è¯
                if not keywords:
                    # å¯»æ‰¾å¯èƒ½æ˜¯å…³é”®è¯çš„çŸ­è¯ç»„
                    words = re.findall(r'[ä¸€-é¾¯]{2,8}', content)
                    keywords = words[:max_keywords]
                
                # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œè¿”å›ä¸€äº›é»˜è®¤å…³é”®è¯
                if not keywords:
                    keywords = ['äº§å“', 'æœåŠ¡', 'è´¨é‡', 'ä½“éªŒ']
                
                result_keywords = keywords[:max_keywords]
                print(f"ğŸ“Š æå–çš„å…³é”®è¯: {result_keywords}")
                return result_keywords
            else:
                print(f"ç™¾åº¦AI Studio APIé”™è¯¯: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"ç™¾åº¦AI Studioå…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    async def calculate_complexity(self, text: str) -> float:
        """è®¡ç®—è¯­è¨€å¤æ‚åº¦"""
        prompt = f"""
        è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬çš„è¯­è¨€å¤æ‚åº¦ï¼Œè¿”å›0åˆ°1ä¹‹é—´çš„æ•°å€¼ï¼š
        0è¡¨ç¤ºéå¸¸ç®€å•ï¼Œ1è¡¨ç¤ºéå¸¸å¤æ‚
        è€ƒè™‘å› ç´ ï¼šå¥å¼å¤æ‚åº¦ã€è¯æ±‡éš¾åº¦ã€é€»è¾‘ç»“æ„ç­‰
        æ–‡æœ¬ï¼š{text}
        """
        
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 10,
                    "temperature": 0.3
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                try:
                    return float(content)
                except ValueError:
                    return 0.5
            else:
                print(f"ç™¾åº¦AI Studio APIé”™è¯¯: {response.status_code}")
                return 0.5
                
        except Exception as e:
            print(f"ç™¾åº¦AI Studioå¤æ‚åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    async def generate_insights(self, dialogue: str, sentiment_score: float, keywords: List[str], complexity_score: float) -> List[str]:
        """ä½¿ç”¨ç™¾åº¦AI Studioç”Ÿæˆæ´å¯Ÿ"""
        try:
            prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†…å®¹ç”Ÿæˆ3-5ä¸ªæ·±åº¦æ´å¯Ÿï¼š
å¯¹è¯å†…å®¹ï¼š{dialogue}
æƒ…æ„Ÿåˆ†æ•°ï¼š{sentiment_score} (èŒƒå›´-1åˆ°1ï¼Œ-1æœ€æ¶ˆæï¼Œ1æœ€ç§¯æ)
å…³é”®è¯ï¼š{', '.join(keywords)}
å¤æ‚åº¦åˆ†æ•°ï¼š{complexity_score} (èŒƒå›´0-1)

è¯·ç”¨ä¸­æ–‡ç”Ÿæˆ3-5ä¸ªæ·±åº¦æ´å¯Ÿï¼Œæ¯ä¸ªæ´å¯Ÿä¸€å¥è¯ï¼Œæ ¼å¼ç®€æ´æ˜äº†ã€‚"""
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 400,
                    "temperature": 0.8
                }
            )
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    print(f"ğŸ” æ´å¯Ÿç”ŸæˆAPIåŸå§‹å“åº”: {result}")
                    content = result["choices"][0]["message"]["content"]
                    print(f"ğŸ” æ´å¯Ÿç”Ÿæˆå†…å®¹: {content}")
                    
                    # è§£ææ´å¯Ÿ
                    insights = []
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#') and not line.startswith('æ´å¯Ÿï¼š'):
                            # ç§»é™¤åºå·å’Œå¸¸è§å‰ç¼€
                            cleaned = line.strip('123456789.-ã€ã€‚Â· ')
                            if cleaned:
                                insights.append(cleaned)
                    
                    if not insights:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰å…¶ä»–æ–¹å¼è§£æ
                        content = content.replace('æ´å¯Ÿï¼š', '').replace('æ´å¯Ÿ:', '')
                        lines = [line.strip() for line in content.split('\n') if line.strip() and not line.startswith('æ´å¯Ÿ')]
                        insights = [line.strip('123456789.-ã€ã€‚Â· ') for line in lines if line.strip()]
                    
                    result_insights = insights[:5] if insights else ["å¯¹è¯åˆ†ææ´å¯Ÿç”Ÿæˆå®Œæˆ"]
                    print(f"ğŸ¤– ç™¾åº¦AIç”Ÿæˆæ´å¯Ÿ: {result_insights}")
                    return result_insights
                except Exception as parse_error:
                    print(f"âŒ æ´å¯Ÿå“åº”è§£æé”™è¯¯: {parse_error}")
                    print(f"âŒ åŸå§‹å“åº”: {result}")
                    return ["AIæ´å¯Ÿç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ´å¯Ÿ"]
            else:
                print(f"âŒ ç™¾åº¦AIæ´å¯Ÿç”ŸæˆAPIé”™è¯¯: {response.status_code}")
                return ["AIæ´å¯Ÿç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ´å¯Ÿ"]
                
        except Exception as e:
            print(f"âŒ ç™¾åº¦AIæ´å¯Ÿç”Ÿæˆé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return ["AIæ´å¯Ÿç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ´å¯Ÿ"]

    async def generate_recommendations(self, dialogue: str, sentiment_score: float, keywords: List[str], complexity_score: float) -> List[str]:
        """ä½¿ç”¨ç™¾åº¦AI Studioç”Ÿæˆå»ºè®®"""
        try:
            prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†…å®¹ç”Ÿæˆ3-5ä¸ªå»ºè®®ï¼š
å¯¹è¯å†…å®¹ï¼š{dialogue}
æƒ…æ„Ÿåˆ†æ•°ï¼š{sentiment_score} (èŒƒå›´-1åˆ°1ï¼Œ-1æœ€æ¶ˆæï¼Œ1æœ€ç§¯æ)
å…³é”®è¯ï¼š{', '.join(keywords)}
å¤æ‚åº¦åˆ†æ•°ï¼š{complexity_score} (èŒƒå›´0-1)

è¯·ç”¨ä¸­æ–‡ç”Ÿæˆ3-5ä¸ªå®ç”¨å»ºè®®ï¼Œæ¯ä¸ªå»ºè®®ä¸€å¥è¯ï¼Œæä¾›å…·ä½“çš„æ”¹è¿›æ–¹å‘ã€‚"""
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 400,
                    "temperature": 0.8
                }
            )
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    print(f"ğŸ” å»ºè®®ç”ŸæˆAPIåŸå§‹å“åº”: {result}")
                    content = result["choices"][0]["message"]["content"]
                    print(f"ğŸ” å»ºè®®ç”Ÿæˆå†…å®¹: {content}")
                    
                    # è§£æå»ºè®®
                    recommendations = []
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#') and not line.startswith('å»ºè®®ï¼š'):
                            # ç§»é™¤åºå·å’Œå¸¸è§å‰ç¼€
                            cleaned = line.strip('123456789.-ã€ã€‚Â· ')
                            if cleaned:
                                recommendations.append(cleaned)
                    
                    if not recommendations:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰å…¶ä»–æ–¹å¼è§£æ
                        content = content.replace('å»ºè®®ï¼š', '').replace('å»ºè®®:', '')
                        lines = [line.strip() for line in content.split('\n') if line.strip() and not line.startswith('å»ºè®®')]
                        recommendations = [line.strip('123456789.-ã€ã€‚Â· ') for line in lines if line.strip()]
                    
                    result_recommendations = recommendations[:5] if recommendations else ["å¯¹è¯æ”¹è¿›å»ºè®®ç”Ÿæˆå®Œæˆ"]
                    print(f"ğŸ¤– ç™¾åº¦AIç”Ÿæˆå»ºè®®: {result_recommendations}")
                    return result_recommendations
                except Exception as parse_error:
                    print(f"âŒ å»ºè®®å“åº”è§£æé”™è¯¯: {parse_error}")
                    print(f"âŒ åŸå§‹å“åº”: {result}")
                    return ["AIå»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å»ºè®®"]
            else:
                print(f"âŒ ç™¾åº¦AIå»ºè®®ç”ŸæˆAPIé”™è¯¯: {response.status_code}")
                return ["AIå»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å»ºè®®"]
                
        except Exception as e:
            print(f"âŒ ç™¾åº¦AIå»ºè®®ç”Ÿæˆé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return ["AIå»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å»ºè®®"]
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


class LLMProviderFactory:
    """LLMæä¾›å•†å·¥å‚"""
    
    @staticmethod
    def create_provider(provider_type: str, **kwargs) -> LLMProvider:
        """åˆ›å»ºLLMæä¾›å•†å®ä¾‹"""
        if provider_type.lower() == "paddle":
            from infrastructure.llm_providers.paddle_provider import PaddleLLMProvider
            client = PaddleLLMProvider()
            if "access_token" in kwargs:
                client.set_access_token(kwargs["access_token"])
            return client
        elif provider_type.lower() == "openai":
            return OpenAIProvider(kwargs.get("api_key", ""))
        elif provider_type.lower() == "wenxin":
            return WenxinProvider(
                api_key=kwargs.get("api_key", ""),
                secret_key=kwargs.get("secret_key", "")
            )
        elif provider_type.lower() == "local":
            return LocalModelProvider(kwargs.get("model_path", ""))
        elif provider_type.lower() == "baidu":
            return BaiduAistudioProvider(kwargs.get("access_token", ""))
        else:
            raise ValueError(f"Unsupported provider type: {provider_type}")