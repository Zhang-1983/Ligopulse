"""
Presentation Layer - API Controllers
è¡¨ç°å±‚ - APIæ§åˆ¶å™¨
"""
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks, File, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
import logging
import tempfile
import os
import asyncio
from pathlib import Path
from datetime import datetime

# å¯¼å…¥å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥å™¨
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from infrastructure.data_importers.wechat_importer import WeChatChatImporter

# å¯¼å…¥å¾®ä¿¡èŠå¤©è®°å½•æå–å™¨
from presentation.wechat_extractor_controller import router as wechat_extractor_router

# å¯¼å…¥LLMæä¾›å•†
from infrastructure.llm_providers.providers import LLMProviderFactory, LLMProvider
from config import get_settings

# è¿™é‡Œéœ€è¦ä»åº”ç”¨å±‚å¯¼å…¥ç”¨ä¾‹å®ç°
# æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦ä» application.usecases å¯¼å…¥


# Pydantic æ¨¡å‹ç”¨äºè¯·æ±‚å’Œå“åº”
class CreateConversationRequest(BaseModel):
    title: str = Field(..., min_length=1, max_length=200, description="å¯¹è¯æ ‡é¢˜")
    conversation_type: str = Field(..., description="å¯¹è¯ç±»å‹: business, casual, academic")
    participants: List[str] = Field(..., min_items=1, max_items=20, description="å‚ä¸è€…åˆ—è¡¨")


class AddTurnRequest(BaseModel):
    content: str = Field(..., min_length=1, max_length=5000, description="å¯¹è¯å†…å®¹")
    speaker_role: str = Field(..., description="è¯´è¯è€…è§’è‰²: interviewer, respondent")


class ConversationResponse(BaseModel):
    id: str
    title: str
    conversation_type: str
    participants: List[str]
    created_at: datetime
    duration_minutes: Optional[int] = None
    turns_count: int = 0


class TurnResponse(BaseModel):
    id: str
    conversation_id: str
    content: str
    speaker_role: str
    timestamp: datetime
    features: Optional[Dict[str, Any]] = None


class AnalysisResponse(BaseModel):
    id: str
    conversation_id: str
    overall_score: float
    peak_intensity: float
    avg_intensity: float
    stability_score: float
    momentum_score: float
    patterns: List[Dict[str, Any]]
    insights: List[str]
    recommendations: List[str]
    pulse_points: List[Dict[str, Any]]
    created_at: datetime


class AnalysisHistoryResponse(BaseModel):
    id: str
    conversation_id: str
    conversation_title: str
    overall_score: float
    pulse_patterns: List[str]
    created_at: datetime
    duration_minutes: int


class ExportReportRequest(BaseModel):
    format_type: str = Field(default="json", description="å¯¼å‡ºæ ¼å¼: json, pdf, csv")


class BatchAnalyzeRequest(BaseModel):
    conversation_ids: List[str] = Field(..., min_items=1, max_items=100, description="è¦åˆ†æçš„å¯¹è¯IDåˆ—è¡¨")
    max_concurrent: int = Field(default=5, ge=1, le=20, description="æœ€å¤§å¹¶å‘æ•°")


class StatusResponse(BaseModel):
    status: str
    message: str
    timestamp: datetime


# API è·¯ç”±å™¨
api_router = APIRouter(prefix="/api/v1", tags=["lingopulse"])

# ä¾èµ–æ³¨å…¥ - å®é™…ä½¿ç”¨æ—¶éœ€è¦é…ç½®
async def get_conversation_use_case():
    # è¿™é‡Œè¿”å›å…·ä½“çš„ç”¨ä¾‹å®ç°å®ä¾‹
    # éœ€è¦æ ¹æ®å®é™…çš„DIå®¹å™¨é…ç½®
    pass

async def get_add_turn_use_case():
    pass

async def get_analyze_conversation_use_case():
    pass

async def get_conversation_history_use_case():
    pass

async def get_analysis_history_use_case():
    pass

async def get_export_report_use_case():
    pass

async def get_batch_analyze_use_case():
    pass


@api_router.post("/conversations", response_model=ConversationResponse, status_code=201)
async def create_conversation(
    request: CreateConversationRequest,
    use_case=Depends(get_conversation_use_case)
):
    """
    åˆ›å»ºæ–°çš„å¯¹è¯
    """
    try:
        # è½¬æ¢è¯·æ±‚ç±»å‹
        from domain.entities import ConversationType
        conversation_type_map = {
            "business": ConversationType.BUSINESS,
            "casual": ConversationType.CASUAL,
            "academic": ConversationType.ACADEMIC
        }
        
        if request.conversation_type not in conversation_type_map:
            raise HTTPException(status_code=400, detail="Invalid conversation type")
        
        conversation = await use_case.execute(
            title=request.title,
            conversation_type=conversation_type_map[request.conversation_type],
            participants=request.participants
        )
        
        return ConversationResponse(
            id=conversation.id,
            title=conversation.title,
            conversation_type=conversation.conversation_type.value,
            participants=conversation.participants,
            created_at=conversation.created_at,
            duration_minutes=conversation.duration_minutes,
            turns_count=len(conversation.turns) if conversation.turns else 0
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create conversation: {str(e)}")


@api_router.post("/conversations/{conversation_id}/turns", response_model=TurnResponse, status_code=201)
async def add_turn(
    conversation_id: str,
    request: AddTurnRequest,
    use_case=Depends(get_add_turn_use_case)
):
    """
    ä¸ºå¯¹è¯æ·»åŠ æ–°çš„è½®æ¬¡
    """
    try:
        # éªŒè¯è¯´è¯è€…è§’è‰²
        from domain.entities import SpeakerRole
        if request.speaker_role not in ["interviewer", "respondent"]:
            raise HTTPException(status_code=400, detail="Invalid speaker role")
        
        speaker_role = SpeakerRole.INTERVIEWER if request.speaker_role == "interviewer" else SpeakerRole.RESPONDENT
        
        turn = await use_case.execute(
            conversation_id=conversation_id,
            content=request.content,
            speaker_role=speaker_role
        )
        
        return TurnResponse(
            id=turn.id,
            conversation_id=turn.conversation_id,
            content=turn.content,
            speaker_role=turn.speaker_role.value,
            timestamp=turn.timestamp,
            features=turn.features.dict() if turn.features else None
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add turn: {str(e)}")


@api_router.get("/conversations/{conversation_id}/analysis", response_model=AnalysisResponse)
async def analyze_conversation(
    conversation_id: str,
    use_case=Depends(get_analyze_conversation_use_case)
):
    """
    åˆ†æå¯¹è¯å¹¶è¿”å›è„‰å†²åˆ†æç»“æœ
    """
    try:
        analysis = await use_case.execute(conversation_id=conversation_id)
        
        return AnalysisResponse(
            id=str(uuid.uuid4()),
            conversation_id=conversation_id,
            overall_score=analysis.overall_score,
            peak_intensity=analysis.peak_intensity,
            avg_intensity=analysis.avg_intensity,
            stability_score=analysis.stability_score,
            momentum_score=analysis.momentum_score,
            patterns=[
                {
                    "name": pattern.name,
                    "description": pattern.description,
                    "confidence": pattern.confidence,
                    "pattern_type": pattern.pattern_type
                }
                for pattern in analysis.patterns
            ],
            insights=analysis.insights,
            recommendations=analysis.recommendations,
            pulse_points=[
                {
                    "timestamp": point.timestamp.isoformat(),
                    "intensity": point.intensity,
                    "sentiment": point.sentiment,
                    "engagement": point.engagement,
                    "clarity": point.clarity,
                    "speaker_role": point.speaker_role.value
                }
                for point in analysis.pulse_points
            ],
            created_at=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to analyze conversation: {str(e)}")


@api_router.get("/conversations", response_model=List[ConversationResponse])
async def get_conversation_history(
    limit: int = 50,
    offset: int = 0,
    use_case=Depends(get_conversation_history_use_case)
):
    """
    è·å–å¯¹è¯å†å²åˆ—è¡¨
    """
    try:
        conversations = await use_case.execute(limit=limit, offset=offset)
        
        return [
            ConversationResponse(
                id=conv.id,
                title=conv.title,
                conversation_type=conv.conversation_type.value,
                participants=conv.participants,
                created_at=conv.created_at,
                duration_minutes=conv.duration_minutes,
                turns_count=len(conv.turns) if conv.turns else 0
            )
            for conv in conversations
        ]
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get conversation history: {str(e)}")


@api_router.get("/conversations/{conversation_id}/turns", response_model=List[TurnResponse])
async def get_conversation_turns(conversation_id: str):
    """
    è·å–å¯¹è¯çš„æ‰€æœ‰è½®æ¬¡
    """
    try:
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»ä»“å‚¨ä¸­è·å–
        # è¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ä½œä¸ºç¤ºä¾‹
        return []
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get conversation turns: {str(e)}")


@api_router.get("/analysis/history", response_model=List[AnalysisHistoryResponse])
async def get_analysis_history(
    limit: int = 20,
    use_case=Depends(get_analysis_history_use_case)
):
    """
    è·å–åˆ†æå†å²è®°å½•
    """
    try:
        history = await use_case.execute(limit=limit)
        
        return [
            AnalysisHistoryResponse(
                id=item["id"],
                conversation_id=item["conversation_id"],
                conversation_title=item["conversation_title"],
                overall_score=item["overall_score"],
                pulse_patterns=item["pulse_patterns"],
                created_at=item["created_at"],
                duration_minutes=item["duration_minutes"]
            )
            for item in history
        ]
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get analysis history: {str(e)}")


@api_router.get("/conversations/{conversation_id}/report")
async def export_analysis_report(
    conversation_id: str,
    format_type: str = "json",
    use_case=Depends(get_export_report_use_case)
):
    """
    å¯¼å‡ºåˆ†ææŠ¥å‘Š
    """
    try:
        if format_type not in ["json", "pdf", "csv"]:
            raise HTTPException(status_code=400, detail="Invalid export format")
        
        report = await use_case.execute(conversation_id=conversation_id, format_type=format_type)
        
        if format_type == "json":
            return JSONResponse(content=report)
        else:
            # å¯¹äº PDF/CSVï¼Œè¿”å›æ–‡ä»¶ä¸‹è½½é“¾æ¥æˆ–æ–‡ä»¶å†…å®¹
            # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›æŠ¥å‘Šæ•°æ®
            return JSONResponse(content={
                "message": f"Report exported in {format_type} format",
                "download_url": f"/api/v1/reports/{conversation_id}.{format_type}",
                "report_data": report
            })
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to export report: {str(e)}")


@api_router.post("/analysis/batch")
async def batch_analyze_conversations(
    request: BatchAnalyzeRequest,
    background_tasks: BackgroundTasks,
    use_case=Depends(get_batch_analyze_use_case)
):
    """
    æ‰¹é‡åˆ†æå¯¹è¯
    """
    try:
        # å¯åŠ¨åå°ä»»åŠ¡
        background_tasks.add_task(
            execute_batch_analysis,
            request.conversation_ids,
            request.max_concurrent
        )
        
        return StatusResponse(
            status="accepted",
            message="Batch analysis started",
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start batch analysis: {str(e)}")


@api_router.get("/analysis/{analysis_id}/status", response_model=StatusResponse)
async def get_analysis_status(analysis_id: str):
    """
    è·å–åˆ†æä»»åŠ¡çŠ¶æ€
    """
    # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»ä»»åŠ¡é˜Ÿåˆ—æˆ–ç¼“å­˜ä¸­è·å–çŠ¶æ€
    return StatusResponse(
        status="completed",
        message="Analysis completed successfully",
        timestamp=datetime.now()
    )


@api_router.post("/analysis/simple")
async def simple_analysis(
    request: dict
):
    """
    ç®€å•åˆ†ææ¥å£ - æ”¯æŒscenarioå’Œdialogueç›´æ¥åˆ†æï¼Œé›†æˆAIæ¨¡å‹å¢å¼ºåˆ†æ
    """
    try:
        scenario = request.get("scenario", "general")
        dialogue = request.get("dialogue", "")
        llm_provider = request.get("llm_provider", "baidu")  # æ–°å¢LLMæä¾›å•†å‚æ•°
        
        if not dialogue:
            raise HTTPException(status_code=400, detail="å¯¹è¯å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # åˆ†æè¿‡ç¨‹ï¼Œå»¶è¿Ÿå‡ ç§’æ¨¡æ‹ŸçœŸå®å¤„ç†
        await asyncio.sleep(2)
        
        # ä½¿ç”¨æŒ‡å®šAIæ¨¡å‹å¢å¼ºçš„æ™ºèƒ½åˆ†æå¯¹è¯å†…å®¹
        analysis_result = await _analyze_conversation_with_ai(dialogue, scenario, llm_provider)
        
        return analysis_result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to analyze: {str(e)}")


async def _analyze_conversation_with_ai(dialogue: str, scenario: str, llm_provider_name: str = "baidu") -> dict:
    """
    ä½¿ç”¨AIæ¨¡å‹å¢å¼ºçš„æ™ºèƒ½åˆ†æå¯¹è¯å†…å®¹
    ç»“åˆä¼ ç»Ÿç®—æ³•å’ŒAIæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæä¾›æ›´å‡†ç¡®ã€ä¸ªæ€§åŒ–çš„åˆ†æç»“æœ
    """
    import random
    import hashlib
    
    # åˆ›å»ºåŸºäºå¯¹è¯å†…å®¹çš„ç§å­ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿä¸€è‡´ç»“æœ
    seed_str = f"{dialogue}_{scenario}"
    seed = int(hashlib.md5(seed_str.encode()).hexdigest()[:8], 16)
    random.seed(seed)
    
    # åŸºç¡€æ–‡æœ¬åˆ†æ
    dialogue_lower = dialogue.lower()
    word_count = len(dialogue.split())
    char_count = len(dialogue)
    
    # ä¼ ç»Ÿå…³é”®è¯æ£€æµ‹
    positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æ„Ÿè°¢', 'åŒæ„', 'æ»¡æ„', 'å–œæ¬¢', 'å¼€å¿ƒ', 'é«˜å…´', 'å¤ªå¥½äº†', 'å®Œç¾', 'èµ', 'å‰å®³', 'æˆåŠŸ', 'ä¼˜ç§€']
    negative_words = ['ä¸å¥½', 'ç³Ÿç³•', 'ä¸åŒæ„', 'å¤±æœ›', 'ç”Ÿæ°”', 'éš¾è¿‡', 'é—®é¢˜', 'å›°éš¾', 'å¤±è´¥', 'é”™è¯¯', 'è®¨åŒ', 'çƒ¦äºº', 'éº»çƒ¦', 'ç—›è‹¦']
    question_words = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å“ªé‡Œ', 'è°', 'å—', 'å‘¢', 'ï¼Ÿ', 'æ˜¯ä¸æ˜¯', 'èƒ½ä¸èƒ½', 'è¦ä¸è¦']
    
    positive_count = sum(1 for word in positive_words if word in dialogue_lower)
    negative_count = sum(1 for word in negative_words if word in dialogue_lower)
    question_count = sum(1 for word in question_words if word in dialogue_lower)
    
    # åˆå§‹åŒ–AIæä¾›å•†è¿›è¡Œæ·±åº¦åˆ†æ
    llm_provider = None
    ai_sentiment_score = None
    ai_keywords = []
    ai_complexity = None
    
    try:
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„LLMæä¾›å•†åˆå§‹åŒ–AIåˆ†æ
        try:
            # æ£€æŸ¥é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„AIæä¾›å•†
            settings = get_settings()
            llm_provider = None
            provider_used = "local"  # è®°å½•å®é™…ä½¿ç”¨çš„æä¾›å•†
            
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æä¾›å•†ç±»å‹åˆ›å»ºå®ä¾‹
            # 1. ä¼˜å…ˆä½¿ç”¨é£æ¡¨å¹³å°
            if llm_provider_name == "paddle" or llm_provider_name == "baidu":
                # é£æ¡¨å¹³å°
                if hasattr(settings, 'paddle_access_token') and settings.paddle_access_token:
                    llm_provider = LLMProviderFactory.create_provider("paddle", 
                        access_token=settings.paddle_access_token)
                    provider_used = "ç™¾åº¦é£æ¡¨å¹³å°"
                    print("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ç™¾åº¦é£æ¡¨å¹³å°AIæä¾›å•†")
                # 2. å…¶æ¬¡ä½¿ç”¨ç™¾åº¦AI Studio
                elif hasattr(settings, 'baidu_access_token') and settings.baidu_access_token:
                    llm_provider = LLMProviderFactory.create_provider("baidu", 
                        access_token=settings.baidu_access_token)
                    provider_used = "ç™¾åº¦AI Studio"
                    print("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ç™¾åº¦AI Studio AIæä¾›å•†")
                else:
                    print("ç™¾åº¦é£æ¡¨å’Œç™¾åº¦AI Studioè®¿é—®ä»¤ç‰Œæœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ")
                    
            elif llm_provider_name == "wenxin":
                # æ–‡å¿ƒä¸€è¨€
                if settings.wenxin_api_key and settings.wenxin_secret_key:
                    llm_provider = LLMProviderFactory.create_provider("wenxin", 
                        api_key=settings.wenxin_api_key, 
                        secret_key=settings.wenxin_secret_key)
                    provider_used = "æ–‡å¿ƒä¸€è¨€"
                    print("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–‡å¿ƒä¸€è¨€AIæä¾›å•†")
                else:
                    print("æ–‡å¿ƒä¸€è¨€APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ")
                    
            # elif llm_provider_name == "openai":
            #     # OpenAI
            #     if settings.openai_api_key:
            #         llm_provider = LLMProviderFactory.create_provider("openai", 
            #             api_key=settings.openai_api_key)
            #         provider_used = "OpenAI"
            #         print("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„OpenAIæä¾›å•†")
            #     else:
            #         print("OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ")
                    
            elif llm_provider_name == "local":
                # æœ¬åœ°æ¨¡å‹
                llm_provider = LLMProviderFactory.create_provider("local", model_path="")
                provider_used = "æœ¬åœ°æ¨¡å‹"
                print("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹")
            
            # å¦‚æœæŒ‡å®šæä¾›å•†ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…¶ä»–å¯ç”¨æä¾›å•†
            if not llm_provider:
                print(f"ç”¨æˆ·é€‰æ‹©çš„{llm_provider_name}æä¾›å•†ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…¶ä»–å¯ç”¨æä¾›å•†")
                
                # æ£€æŸ¥å…¶ä»–å¯ç”¨æä¾›å•†ï¼Œä¼˜å…ˆä½¿ç”¨ç™¾åº¦ç›¸å…³æŠ€æœ¯
                # 1. é¦–é€‰ç™¾åº¦é£æ¡¨å¹³å°
                if hasattr(settings, 'paddle_access_token') and settings.paddle_access_token:
                    llm_provider = LLMProviderFactory.create_provider("paddle", 
                        access_token=settings.paddle_access_token)
                    provider_used = "ç™¾åº¦é£æ¡¨å¹³å°(å›é€€)"
                    print("å›é€€ä½¿ç”¨ç™¾åº¦é£æ¡¨å¹³å°AIæä¾›å•†")
                # 2. å…¶æ¬¡ä½¿ç”¨ç™¾åº¦AI Studio
                elif hasattr(settings, 'baidu_access_token') and settings.baidu_access_token:
                    llm_provider = LLMProviderFactory.create_provider("baidu", 
                        access_token=settings.baidu_access_token)
                    provider_used = "ç™¾åº¦AI Studio(å›é€€)"
                    print("å›é€€ä½¿ç”¨ç™¾åº¦AI Studio AIæä¾›å•†")
                # 3. å…¶ä»–ç™¾åº¦ç›¸å…³æœåŠ¡
                elif settings.wenxin_api_key and settings.wenxin_secret_key:
                    llm_provider = LLMProviderFactory.create_provider("wenxin", 
                        api_key=settings.wenxin_api_key, 
                        secret_key=settings.wenxin_secret_key)
                    provider_used = "æ–‡å¿ƒä¸€è¨€(å›é€€)"
                    print("å›é€€ä½¿ç”¨æ–‡å¿ƒä¸€è¨€AIæä¾›å•†")
                # 4. éç™¾åº¦æœåŠ¡ä½œä¸ºæœ€åçš„é€‰æ‹©
                elif settings.openai_api_key:
                    llm_provider = LLMProviderFactory.create_provider("openai", 
                        api_key=settings.openai_api_key)
                    provider_used = "OpenAI(å›é€€)"
                    print("å›é€€ä½¿ç”¨OpenAIæä¾›å•†")
                else:
                    # æœ€åå›é€€åˆ°æœ¬åœ°æ¨¡æ‹Ÿ
                    llm_provider = LLMProviderFactory.create_provider("local", model_path="")
                    provider_used = "æœ¬åœ°æ¨¡æ‹Ÿ(å›é€€)"
                    print("æ‰€æœ‰APIæä¾›å•†éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ‹ŸAIæä¾›å•†")
            
            # è®°å½•å®é™…ä½¿ç”¨çš„æä¾›å•†
            print(f"âœ… å®é™…ä½¿ç”¨AIæä¾›å•†: {provider_used}")
            
            # å¹¶è¡Œæ‰§è¡ŒAIåˆ†æä»»åŠ¡
            tasks = []
            
            # 1. AIæƒ…æ„Ÿåˆ†æ
            tasks.append(llm_provider.analyze_sentiment(dialogue))
            
            # 2. AIå…³é”®è¯æå–
            tasks.append(llm_provider.extract_keywords(dialogue, max_keywords=8))
            
            # 3. AIå¤æ‚åº¦åˆ†æ
            tasks.append(llm_provider.calculate_complexity(dialogue))
            
            # æ‰§è¡Œæ‰€æœ‰AIåˆ†æä»»åŠ¡
            ai_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†AIç»“æœ - ç¡®ä¿æ‰€æœ‰å˜é‡éƒ½æœ‰é»˜è®¤å€¼
            ai_sentiment_score = None
            ai_keywords = []
            ai_complexity = None
            
            if isinstance(ai_results[0], (int, float)):
                ai_sentiment_score = ai_results[0]
                print(f"ğŸ¤– AIæƒ…æ„Ÿåˆ†æç»“æœ: {ai_sentiment_score}")
            
            if isinstance(ai_results[1], list):
                ai_keywords = ai_results[1]
                print(f"ğŸ” AIå…³é”®è¯æå–ç»“æœ: {ai_keywords}")
                
            if isinstance(ai_results[2], (int, float)):
                ai_complexity = ai_results[2]
                print(f"ğŸ“Š AIå¤æ‚åº¦åˆ†æç»“æœ: {ai_complexity}")
                
            # å¤„ç†å¯èƒ½çš„å¼‚å¸¸æƒ…å†µ
            if isinstance(ai_results[0], Exception):
                print(f"âŒ AIæƒ…æ„Ÿåˆ†æå¤±è´¥: {ai_results[0]}")
            if isinstance(ai_results[1], Exception):
                print(f"âŒ AIå…³é”®è¯æå–å¤±è´¥: {ai_results[1]}")
            if isinstance(ai_results[2], Exception):
                print(f"âŒ AIå¤æ‚åº¦è®¡ç®—å¤±è´¥: {ai_results[2]}")
                
        except Exception as ai_error:
            print(f"AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {ai_error}")
            # å¦‚æœAIå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çš„æƒ…æ„Ÿåˆ†æ•°
            ai_sentiment_score = (positive_count - negative_count) / max(1, word_count / 10)
        
        # èåˆAIå’Œä¼ ç»Ÿåˆ†æç»“æœ
        if ai_sentiment_score is not None:
            # ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æ•°
            traditional_sentiment = (positive_count - negative_count) / max(1, word_count / 10)
            # èåˆæƒé‡ï¼šAIæ¨¡å‹70% + ä¼ ç»Ÿæ–¹æ³•30%
            sentiment_score = 0.7 * ai_sentiment_score + 0.3 * traditional_sentiment
        else:
            # ä»…ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            sentiment_score = (positive_count - negative_count) / max(1, word_count / 10)
        
        # é™åˆ¶åœ¨-1åˆ°1ä¹‹é—´
        sentiment_score = max(-1, min(1, sentiment_score))
        
        # å¦‚æœAIå¤æ‚åº¦å¯ç”¨ï¼Œä½¿ç”¨AIç»“æœï¼Œå¦åˆ™ä¼°ç®—
        if ai_complexity is not None:
            complexity_score = ai_complexity
        else:
            # åŸºäºå¯¹è¯é•¿åº¦å’Œå¥å­ç»“æ„ä¼°ç®—å¤æ‚åº¦
            sentences = dialogue.split('ã€‚') + dialogue.split('.') + dialogue.split('!') + dialogue.split('?')
            if sentences:
                avg_length = sum(len(s.split()) for s in sentences) / len(sentences)
                complexity_score = min(avg_length / 20, 1.0)
            else:
                complexity_score = 0.5
        
        # è®¡ç®—å‚ä¸åº¦ï¼ˆåŸºäºå¯¹è¯é•¿åº¦å’Œé—®ç­”æ¯”ä¾‹ï¼‰
        engagement_base = min(1.0, word_count / 100)  # å¯¹è¯é•¿åº¦å› å­
        question_factor = min(1.0, question_count / max(1, word_count / 20))  # é—®ç­”äº’åŠ¨å› å­
        engagement = (engagement_base + question_factor) / 2
        
        # AIå¢å¼ºçš„æ•´ä½“åˆ†æ•°è®¡ç®—
        # åŸºç¡€åˆ†æ•° + AIå¤æ‚åº¦å› å­ + å‚ä¸åº¦æƒé‡
        base_score = 0.3
        sentiment_factor = 0.3 * (sentiment_score + 1) / 2  # è½¬æ¢ä¸º0-1èŒƒå›´
        engagement_factor = 0.4 * engagement
        complexity_factor = 0.1 * complexity_score  # é€‚åº¦çš„å¤æ‚åº¦å¥–åŠ±
        
        overall_score = round(min(1.0, base_score + sentiment_factor + engagement_factor + complexity_factor), 2)
        
        # è®¡ç®—å¼ºåº¦æŒ‡æ ‡ï¼ˆAIå¢å¼ºï¼‰
        ai_influence = 0.3 if ai_sentiment_score is not None else 0.1
        avg_intensity = round(0.3 + 0.4 * engagement + 0.3 * (sentiment_score + 1) / 2 + ai_influence * complexity_score, 2)
        peak_intensity = round(min(1.0, avg_intensity + 0.3 * random.random()), 2)
        stability_score = round(1.0 - abs(sentiment_score) * 0.5 + 0.2 * random.random(), 2)
        momentum_score = round(0.4 + 0.4 * engagement + 0.2 * (1 - stability_score), 2)
        
        # ç”ŸæˆåŠ¨æ€æ¨¡å¼
        patterns = _generate_patterns(scenario, sentiment_score, question_count, word_count)
        
        # ç”Ÿæˆæ´å¯Ÿå’Œå»ºè®® - ä½¿ç”¨AIå¢å¼ºç‰ˆæœ¬
        ai_insights = []
        ai_recommendations = []
        
        print(f"ğŸ” æ£€æŸ¥AIæä¾›å•†: {llm_provider is not None}")
        
        if llm_provider:
            try:
                # å°è¯•ä½¿ç”¨AIç”Ÿæˆæ´å¯Ÿå’Œå»ºè®®
                print("ğŸ¤– å¼€å§‹ä½¿ç”¨AIç”Ÿæˆæ·±åº¦æ´å¯Ÿå’Œå»ºè®®...")
                print(f"ğŸ“ å‘é€ç»™AIçš„å‚æ•° - æƒ…æ„Ÿåˆ†æ•°: {ai_sentiment_score or sentiment_score:.3f}")
                print(f"ğŸ”‘ AIå…³é”®è¯: {ai_keywords[:5] if ai_keywords else 'None'}")
                print(f"ğŸ§® å¤æ‚åº¦åˆ†æ•°: {ai_complexity or 0.5:.3f}")
                
                # å¹¶è¡Œç”Ÿæˆæ´å¯Ÿå’Œå»ºè®®
                insight_task = llm_provider.generate_insights(dialogue, ai_sentiment_score or sentiment_score, ai_keywords, ai_complexity or 0.5)
                recommendation_task = llm_provider.generate_recommendations(dialogue, ai_sentiment_score or sentiment_score, ai_keywords, ai_complexity or 0.5)
                
                print("â³ ç­‰å¾…AIæ´å¯Ÿå’Œå»ºè®®ç”Ÿæˆ...")
                # ç­‰å¾…AIç”Ÿæˆç»“æœ
                ai_insights, ai_recommendations = await asyncio.gather(insight_task, recommendation_task)
                
                # éªŒè¯AIç»“æœ
                if ai_insights:
                    print(f"âœ… AIæˆåŠŸç”Ÿæˆ{len(ai_insights)}ä¸ªæ´å¯Ÿ: {ai_insights[:2]}...")
                else:
                    ai_insights = ["AIæ´å¯Ÿç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ´å¯Ÿ"]
                    
                if ai_recommendations:
                    print(f"âœ… AIæˆåŠŸç”Ÿæˆ{len(ai_recommendations)}ä¸ªå»ºè®®: {ai_recommendations[:2]}...")
                else:
                    ai_recommendations = ["AIå»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿå»ºè®®"]
                    
            except Exception as ai_error:
                print(f"âš ï¸ AIæ´å¯Ÿå’Œå»ºè®®ç”Ÿæˆå¤±è´¥: {ai_error}")
                ai_insights = ["ç½‘ç»œé”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€æ´å¯Ÿ"]
                ai_recommendations = ["ç½‘ç»œé”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€å»ºè®®"]
        else:
            print("âš ï¸ AIæä¾›å•†æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        
        # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        if not ai_insights:
            print("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆæ´å¯Ÿ")
            ai_insights = _generate_insights(
                scenario=scenario,
                dialogue=dialogue,
                sentiment_score=sentiment_score,
                question_count=question_count,
                engagement=engagement,
                word_count=word_count
            )
            
        if not ai_recommendations:
            print("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå»ºè®®")
            ai_recommendations = _generate_recommendations(
                scenario=scenario,
                dialogue=dialogue,
                sentiment_score=sentiment_score,
                question_count=question_count,
                engagement=engagement
            )
        
        # ç”Ÿæˆè„‰å†²ç‚¹
        pulse_points = _generate_pulse_points(word_count, sentiment_score, engagement)
        
        # æ„å»ºå¢å¼ºçš„åˆ†æç»“æœ
        analysis_result = {
            "id": str(uuid.uuid4()),
            "scenario": scenario,
            "overall_score": overall_score,
            "peak_intensity": peak_intensity,
            "avg_intensity": avg_intensity,
            "stability_score": stability_score,
            "momentum_score": momentum_score,
            "complexity_score": round(complexity_score, 2),
            "patterns": patterns,
            "insights": ai_insights,
            "recommendations": ai_recommendations,
            "pulse_points": pulse_points,
            "ai_analysis": {
                "sentiment_score": round(ai_sentiment_score, 3) if ai_sentiment_score is not None else round(sentiment_score, 3),
                "keywords": ai_keywords[:5] if ai_keywords else [],  # è¿”å›å‰5ä¸ªAIæå–çš„å…³é”®è¯
                "complexity_score": round(ai_complexity, 3) if ai_complexity is not None else round(complexity_score, 3),
                "enhancement_applied": ai_sentiment_score is not None or len(ai_keywords) > 0
            },
            "created_at": datetime.now()
        }
        
        # å…³é—­AIæä¾›å•†è¿æ¥
        if llm_provider:
            await llm_provider.close()
        
        return analysis_result
        
    except Exception as e:
        print(f"AIå¢å¼ºåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
        # å¦‚æœAIå¢å¼ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        return _analyze_conversation(dialogue, scenario)


def _analyze_conversation(dialogue: str, scenario: str) -> dict:
    """
    æ™ºèƒ½åˆ†æå¯¹è¯å†…å®¹
    """
    import random
    import hashlib
    import re
    
    # åˆ›å»ºåŸºäºå¯¹è¯å†…å®¹çš„ç§å­ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿä¸€è‡´ç»“æœ
    seed_str = f"{dialogue}_{scenario}"
    seed = int(hashlib.md5(seed_str.encode()).hexdigest()[:8], 16)
    random.seed(seed)
    
    # åˆ†æå¯¹è¯ç‰¹å¾
    dialogue_lower = dialogue.lower()
    word_count = len(dialogue.split())
    char_count = len(dialogue)
    
    # æ£€æµ‹æƒ…æ„Ÿå…³é”®è¯
    positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æ„Ÿè°¢', 'åŒæ„', 'æ»¡æ„', 'å–œæ¬¢', 'å¼€å¿ƒ', 'é«˜å…´', 'å¤ªå¥½äº†', 'å®Œç¾']
    negative_words = ['ä¸å¥½', 'ç³Ÿç³•', 'ä¸åŒæ„', 'å¤±æœ›', 'ç”Ÿæ°”', 'éš¾è¿‡', 'é—®é¢˜', 'å›°éš¾', 'å¤±è´¥', 'é”™è¯¯']
    question_words = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å“ªé‡Œ', 'è°', 'å—', 'å‘¢', 'ï¼Ÿ']
    
    positive_count = sum(1 for word in positive_words if word in dialogue_lower)
    negative_count = sum(1 for word in negative_words if word in dialogue_lower)
    question_count = sum(1 for word in question_words if word in dialogue_lower)
    
    # è®¡ç®—æƒ…æ„Ÿå€¾å‘
    sentiment_score = (positive_count - negative_count) / max(1, word_count / 10)
    sentiment_score = max(-1, min(1, sentiment_score))  # é™åˆ¶åœ¨-1åˆ°1ä¹‹é—´
    
    # è®¡ç®—å‚ä¸åº¦ï¼ˆåŸºäºå¯¹è¯é•¿åº¦å’Œé—®ç­”æ¯”ä¾‹ï¼‰
    engagement_base = min(1.0, word_count / 100)  # å¯¹è¯é•¿åº¦å› å­
    question_factor = min(1.0, question_count / max(1, word_count / 20))  # é—®ç­”äº’åŠ¨å› å­
    engagement = (engagement_base + question_factor) / 2
    
    # è®¡ç®—æ•´ä½“åˆ†æ•°
    overall_score = round(0.3 + 0.4 * engagement + 0.3 * (sentiment_score + 1) / 2, 2)
    
    # è®¡ç®—å¼ºåº¦æŒ‡æ ‡
    avg_intensity = round(0.3 + 0.4 * engagement + 0.3 * random.random(), 2)
    peak_intensity = round(min(1.0, avg_intensity + 0.3 * random.random()), 2)
    stability_score = round(1.0 - abs(sentiment_score) * 0.5 + 0.2 * random.random(), 2)
    momentum_score = round(0.4 + 0.4 * engagement + 0.2 * (1 - stability_score), 2)
    
    # ç”ŸæˆåŠ¨æ€æ¨¡å¼
    patterns = _generate_patterns(scenario, sentiment_score, question_count, word_count)
    
    # ç”Ÿæˆæ´å¯Ÿ
    insights = _generate_insights(
        scenario=scenario,
        dialogue=dialogue,
        sentiment_score=sentiment_score,
        question_count=question_count,
        engagement=engagement,
        word_count=word_count
    )
    
    # ç”Ÿæˆå»ºè®®
    recommendations = _generate_recommendations(
        scenario=scenario,
        dialogue=dialogue,
        sentiment_score=sentiment_score,
        question_count=question_count,
        engagement=engagement
    )
    
    # ç”Ÿæˆè„‰å†²ç‚¹
    pulse_points = _generate_pulse_points(word_count, sentiment_score, engagement)
    
    return {
        "id": str(uuid.uuid4()),
        "scenario": scenario,
        "overall_score": overall_score,
        "peak_intensity": peak_intensity,
        "avg_intensity": avg_intensity,
        "stability_score": stability_score,
        "momentum_score": momentum_score,
        "patterns": patterns,
        "insights": insights,
        "recommendations": recommendations,
        "pulse_points": pulse_points,
        "created_at": datetime.now()
    }


def _generate_patterns(scenario: str, sentiment_score: float, question_count: int, word_count: int) -> list:
    """ç”ŸæˆåŠ¨æ€æ¨¡å¼"""
    patterns = []
    
    # åŸºäºæƒ…æ„Ÿå€¾å‘çš„æ¨¡å¼
    if sentiment_score > 0.3:
        patterns.append({
            "name": "ç§¯æäº’åŠ¨æ¨¡å¼",
            "description": "å¯¹è¯ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„äº’åŠ¨æ€§å’Œç§¯æçš„æ²Ÿé€šæ°›å›´",
            "confidence": round(0.7 + 0.3 * sentiment_score, 2),
            "pattern_type": "communication"
        })
    elif sentiment_score < -0.3:
        patterns.append({
            "name": "æ¶ˆææƒ…ç»ªæ¨¡å¼",
            "description": "å¯¹è¯ä¸­å¯èƒ½å­˜åœ¨ä¸€äº›è´Ÿé¢æƒ…ç»ªæˆ–æŒ‘æˆ˜æ€§è¯é¢˜",
            "confidence": round(0.6 + 0.3 * abs(sentiment_score), 2),
            "pattern_type": "emotional"
        })
    
    # åŸºäºé—®ç­”äº’åŠ¨çš„æ¨¡å¼
    if question_count > 0:
        engagement_level = min(1.0, question_count / max(1, word_count / 50))
        patterns.append({
            "name": "é—®ç­”äº’åŠ¨æ¨¡å¼",
            "description": "å¯¹è¯ä¸­åŒ…å«è¾ƒå¤šçš„é—®ç­”äº’åŠ¨ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‚ä¸åº¦",
            "confidence": round(0.6 + 0.4 * engagement_level, 2),
            "pattern_type": "engagement"
        })
    
    # åŸºäºåœºæ™¯çš„æ¨¡å¼
    if scenario in ["é¢è¯•", "presentation", "æ¼”è®²"]:
        patterns.append({
            "name": "æ­£å¼æ²Ÿé€šæ¨¡å¼",
            "description": "åœ¨æ­£å¼åœºåˆä¸‹çš„ä¸“ä¸šæ²Ÿé€šæ¨¡å¼",
            "confidence": 0.8,
            "pattern_type": "formal"
        })
    elif scenario in ["èŠå¤©", "æ—¥å¸¸å¯¹è¯"]:
        patterns.append({
            "name": "æ—¥å¸¸äº¤æµæ¨¡å¼",
            "description": "è½»æ¾å‹å¥½çš„æ—¥å¸¸äº¤æµæ¨¡å¼",
            "confidence": 0.75,
            "pattern_type": "casual"
        })
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ¨¡å¼ï¼Œæ·»åŠ é»˜è®¤æ¨¡å¼
    if not patterns:
        patterns.append({
            "name": "ä¸€èˆ¬å¯¹è¯æ¨¡å¼",
            "description": "æ ‡å‡†çš„ä¸­æ€§å¯¹è¯æ¨¡å¼",
            "confidence": 0.6,
            "pattern_type": "neutral"
        })
    
    return patterns


def _generate_insights(scenario: str, dialogue: str, sentiment_score: float, question_count: int, engagement: float, word_count: int) -> list:
    """ç”Ÿæˆä¸ªæ€§åŒ–çš„æ·±åº¦æ´å¯Ÿ"""
    insights = []
    
    # æƒ…æ„Ÿæ·±åº¦åˆ†æ
    if sentiment_score > 0.5:
        insights.append("ğŸ‰ å¯¹è¯å……æ»¡æ­£èƒ½é‡ï¼ŒåŒæ–¹è¡¨ç°å‡ºé«˜åº¦çš„çƒ­æƒ…å’Œç§¯ææ€§ï¼Œå¯èƒ½å»ºç«‹äº†è‰¯å¥½çš„æƒ…æ„Ÿè¿æ¥")
    elif sentiment_score > 0.2:
        insights.append("ğŸ˜Š å¯¹è¯æ•´ä½“æ°›å›´å‹å¥½æ­£å‘ï¼Œå‚ä¸è€…ä¿æŒäº†ç§¯æçš„æ€åº¦å’Œåˆä½œç²¾ç¥")
    elif sentiment_score < -0.5:
        insights.append("ğŸ˜° å¯¹è¯ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„æ¶ˆææƒ…ç»ªï¼Œå¯èƒ½å­˜åœ¨åˆ†æ­§ã€æŒ«æŠ˜æˆ–ä¸æ»¡æƒ…ç»ªéœ€è¦å…³æ³¨")
    elif sentiment_score < -0.2:
        insights.append("ğŸ˜ å¯¹è¯å­˜åœ¨ä¸€å®šçš„ç´§å¼ æ„Ÿæˆ–ä¸ç¡®å®šæ€§ï¼Œå»ºè®®åŠ å¼ºæ²Ÿé€šå’Œç†è§£")
    else:
        insights.append("âš–ï¸ å¯¹è¯ä¿æŒç†æ€§å¹³è¡¡çš„çŠ¶æ€ï¼Œå‚ä¸è€…ä»¥å®¢è§‚ã€å†·é™çš„æ€åº¦è¿›è¡Œäº¤æµ")
    
    # å‚ä¸åº¦è´¨é‡åˆ†æ
    if engagement > 0.8:
        insights.append("ğŸ”¥ åŒæ–¹æ·±åº¦æŠ•å…¥ï¼Œå¯¹è¯äº’åŠ¨é¢‘ç¹ä¸”å¯Œæœ‰æˆæ•ˆï¼Œå¯èƒ½æ¶‰åŠé‡è¦è¯é¢˜")
    elif engagement > 0.6:
        insights.append("ğŸ’¬ å¯¹è¯å‚ä¸ç§¯æï¼ŒåŒæ–¹éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ²Ÿé€šæ„æ„¿å’Œäº’åŠ¨æŠ€å·§")
    elif engagement > 0.4:
        insights.append("ğŸ¤ å‚ä¸è€…ä¿æŒé€‚åº¦çš„å‚ä¸åº¦ï¼Œå¯¹è¯èŠ‚å¥æ§åˆ¶å¾—å½“")
    else:
        insights.append("ğŸ˜´ å¯¹è¯å‚ä¸åº¦åä½ï¼Œå¯èƒ½å­˜åœ¨ä¿¡æ¯ä¸å¯¹ç§°æˆ–å…´è¶£ä¸åŒ¹é…çš„æƒ…å†µ")
    
    # å¯¹è¯ç»“æ„åˆ†æ
    if word_count > 200:
        insights.append("ğŸ“š å¯¹è¯å†…å®¹ä¸°å¯Œè¯¦å®ï¼Œå¯èƒ½æ¶‰åŠå¤æ‚è¯é¢˜çš„æ·±å…¥è®¨è®º")
    elif word_count > 100:
        insights.append("ğŸ’­ å¯¹è¯å†…å®¹é€‚ä¸­ï¼Œæ—¢ä¿è¯äº†ä¿¡æ¯ä¼ é€’åˆç»´æŒäº†è‰¯å¥½çš„æ²Ÿé€šæ•ˆç‡")
    else:
        insights.append("ğŸ’¬ å¯¹è¯ç®€æ´æ˜å¿«ï¼Œä¼ é€’æ ¸å¿ƒä¿¡æ¯ï¼Œå¯èƒ½æ˜¯é«˜æ•ˆçš„å†³ç­–æˆ–ç¡®è®¤æ€§æ²Ÿé€š")
    
    # é—®ç­”æ¨¡å¼æ·±åº¦åˆ†æ
    if question_count > word_count / 15:
        insights.append("ğŸ¤” å¯¹è¯ä»¥é—®é¢˜é©±åŠ¨ä¸ºä¸»ï¼Œå±•ç°å‡ºæ¢ç´¢æ€§å’Œå­¦ä¹ æ€§çš„äº¤æµç‰¹ç‚¹")
        insights.append("ğŸ’¡ æé—®è€…è¡¨ç°å‡ºå¼ºçƒˆçš„å¥½å¥‡å¿ƒå’Œæ±‚çŸ¥æ¬²ï¼Œè¿™æ˜¯æœ‰æ•ˆæ²Ÿé€šçš„é‡è¦æ ‡å¿—")
    elif question_count > word_count / 25:
        insights.append("â“ å¯¹è¯å¹³è¡¡äº†æé—®å’Œé™ˆè¿°ï¼Œäº’åŠ¨èŠ‚å¥è‰¯å¥½")
    elif question_count == 0:
        insights.append("ğŸ“¢ å¯¹è¯ä»¥ä¿¡æ¯ä¼ é€’ä¸ºä¸»ï¼Œå¯èƒ½æ˜¯å•å‘çš„è¯´æ˜æˆ–æŠ¥å‘Šæ€§æ²Ÿé€š")
    
    # åœºæ™¯ç‰¹å®šçš„æ·±åº¦æ´å¯Ÿ
    if scenario == "é¢è¯•":
        if sentiment_score > 0.3:
            insights.append("ğŸ¯ é¢è¯•æ²Ÿé€šä¸“ä¸šä¸”ç§¯æï¼Œå€™é€‰äººå±•ç°äº†è‰¯å¥½çš„æ²Ÿé€šæŠ€å·§å’Œè‡ªä¿¡å¿ƒ")
        else:
            insights.append("ğŸ“‹ é¢è¯•å¯¹è¯ç›¸å¯¹æ­£å¼ï¼Œå¯èƒ½éœ€è¦å¢åŠ æ›´å¤šäº’åŠ¨æ€§æ¥å±•ç¤ºä¸ªäººé­…åŠ›")
    elif scenario == "èŠå¤©":
        if engagement > 0.6:
            insights.append("ğŸ—£ï¸ èŠå¤©æ°›å›´è½»æ¾è‡ªç„¶ï¼ŒåŒæ–¹å»ºç«‹äº†è‰¯å¥½çš„å¯¹è¯é»˜å¥‘å’Œæƒ…æ„Ÿè¿æ¥")
        else:
            insights.append("ğŸ˜Œ èŠå¤©é£æ ¼æ¸©å’Œç†æ€§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šå…±åŒè¯é¢˜æ¥æå‡äº’åŠ¨è´¨é‡")
    elif scenario == "ä¼šè®®":
        insights.append("ğŸ¢ å¯¹è¯ä½“ç°äº†å›¢é˜Ÿåä½œçš„ç‰¹ç‚¹ï¼Œå‚ä¸è€…å±•ç°äº†ä¸“ä¸šæ€§å’Œåˆä½œç²¾ç¥")
    elif scenario == "å’¨è¯¢":
        insights.append("ğŸ§  å’¨è¯¢å¯¹è¯æ˜¾ç¤ºå‡ºä¸“ä¸šæ€§å’Œå»ºè®¾æ€§ï¼Œå¯èƒ½æ˜¯çŸ¥è¯†åˆ†äº«æˆ–é—®é¢˜è§£å†³çš„æœ‰æ•ˆæ²Ÿé€š")
    
    # è¯­è¨€ä½¿ç”¨åˆ†æ
    positive_words = ["è°¢è°¢", "æ„Ÿè°¢", "å¥½", "æ£’", "å–œæ¬¢", "æ»¡æ„", "åŒæ„", "æ”¯æŒ"]
    negative_words = ["ä¸å¥½", "è®¨åŒ", "å¤±æœ›", "ä¸æ»¡", "åå¯¹", "æ‹’ç»", "é—®é¢˜", "å›°éš¾"]
    question_words = ["ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "èƒ½å¦", "æ˜¯å¦"]
    
    positive_count = sum(1 for word in positive_words if word in dialogue)
    negative_count = sum(1 for word in negative_words if word in dialogue)
    question_count_detailed = sum(1 for word in question_words if word in dialogue)
    
    if positive_count > negative_count * 1.5:
        insights.append("âœ¨ å¯¹è¯ä¸­ç§¯æè¯æ±‡ä½¿ç”¨é¢‘ç¹ï¼Œåæ˜ äº†ä¹è§‚æ­£é¢çš„æ²Ÿé€šå¿ƒæ€")
    elif negative_count > positive_count * 1.5:
        insights.append("âš ï¸ å¯¹è¯ä¸­æ¶ˆæè¯æ±‡è¾ƒå¤šï¼Œå¯èƒ½åæ˜ äº†å½“å‰çš„æŒ‘æˆ˜æˆ–å›°éš¾éœ€è¦å…³æ³¨")
    
    if question_count_detailed > 3:
        insights.append("â“ å¯¹è¯åŒ…å«å¤šä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼Œå±•ç°äº†æ·±å…¥æ€è€ƒå’Œæ¢ç´¢çš„å€¾å‘")
    
    return insights


def _generate_recommendations(scenario: str, dialogue: str, sentiment_score: float, question_count: int, engagement: float) -> list:
    """ç”Ÿæˆä¸ªæ€§åŒ–çš„æ·±åº¦å»ºè®®"""
    recommendations = []
    word_count = len(dialogue.split())
    
    # æƒ…æ„Ÿä¼˜åŒ–çš„ç²¾ç»†å»ºè®®
    if sentiment_score < -0.6:
        recommendations.append("ğŸš¨ ç´§æ€¥å»ºè®®ï¼šå¯¹è¯å­˜åœ¨æ˜æ˜¾çš„è´Ÿé¢æƒ…ç»ªï¼Œå»ºè®®ç«‹å³æš‚åœäº‰è®®æ€§è¯é¢˜ï¼Œè½¬æ¢ä¸ºä¸­ç«‹è®¨è®º")
        recommendations.append("ğŸ’ å¯ä»¥å°è¯•è¡¨è¾¾ç†è§£å’Œå…±æƒ…ï¼Œå¦‚'æˆ‘ç†è§£ä½ çš„æ‹…å¿§'ï¼ŒåŒ–è§£ç´§å¼ æ°›å›´")
        recommendations.append("ğŸ¤ å»ºè®®é‡æ–°èšç„¦å…±åŒç›®æ ‡å’Œä»·å€¼è§‚ï¼Œå¯»æ‰¾å…±è¯†ç‚¹")
    elif sentiment_score < -0.3:
        recommendations.append("âš–ï¸ å¯¹è¯ç•¥æ˜¾ç´§å¼ ï¼Œå»ºè®®é‡‡ç”¨æ›´æ¸©å’Œçš„æªè¾å’Œè¯­é€Ÿï¼Œé™ä½å†²çªé£é™©")
        recommendations.append("ğŸ‘‚ åŠ å¼ºä¸»åŠ¨å€¾å¬æŠ€å·§ï¼Œé€šè¿‡å¤è¿°å¯¹æ–¹è§‚ç‚¹æ¥æ˜¾ç¤ºç†è§£å’Œå°Šé‡")
        recommendations.append("ğŸ”„ é€‚æ—¶è½¬æ¢è¯é¢˜ï¼Œé¿å…åœ¨æ•æ„Ÿç‚¹ä¸Šè¿‡åº¦çº ç¼ ")
    elif sentiment_score > 0.6:
        recommendations.append("ğŸ‰ ä¼˜ç§€è¡¨ç°ï¼šä¿æŒå½“å‰çš„æ­£å‘æ²Ÿé€šé£æ ¼ï¼Œè¿™æ˜¯å»ºç«‹ä¿¡ä»»çš„åŸºç¡€")
        recommendations.append("ğŸŒŸ å¯ä»¥é€‚åº¦åˆ†äº«ä¸ªäººæ„Ÿå—å’Œç»éªŒï¼Œå¢å¼ºå¯¹è¯çš„æ·±åº¦å’ŒçœŸå®æ€§")
        recommendations.append("ğŸš€ å€Ÿæ­¤ç§¯ææ°›å›´ï¼Œå¯ä»¥è®¨è®ºæ›´å…·æœ‰æŒ‘æˆ˜æ€§æˆ–åˆ›æ–°æ€§çš„è®®é¢˜")
    elif sentiment_score > 0.3:
        recommendations.append("ğŸ˜Š ä¿æŒç°æœ‰çš„ç§¯ææ²Ÿé€šæ–¹å¼ï¼Œè¿™å°†æœ‰åŠ©äºç»´æŒè‰¯å¥½çš„å…³ç³»")
        recommendations.append("ğŸ“ˆ å¯ä»¥é€‚å½“è¯¢é—®å¯¹æ–¹çš„æƒ³æ³•å’Œæ„Ÿå—ï¼Œä¿ƒè¿›åŒå‘äº¤æµ")
    
    # å‚ä¸åº¦ä¼˜åŒ–çš„ç²¾å‡†å»ºè®®
    if engagement < 0.3:
        recommendations.append("ğŸ˜´ å¯¹è¯å‚ä¸åº¦è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨å¼€æ”¾å¼é—®é¢˜å¯åŠ¨å¯¹è¯")
        recommendations.append("ğŸ¯ å°è¯•åˆ†äº«å…·ä½“æ¡ˆä¾‹æˆ–æ•…äº‹æ¥æ¿€å‘å¯¹æ–¹çš„å…´è¶£å’Œå‚ä¸")
        recommendations.append("â“ ç›´æ¥è¯¢é—®'ä½ å¯¹è¿™ä¸ªé—®é¢˜æ€ä¹ˆçœ‹ï¼Ÿ'ç­‰æ˜ç¡®é‚€è¯·å‚ä¸çš„è¡¨è¾¾")
    elif engagement < 0.5:
        recommendations.append("ğŸ“ å¯ä»¥é€šè¿‡æ€»ç»“å’Œæ¾„æ¸…æ¥å¢åŠ äº’åŠ¨ï¼š'è®©æˆ‘ç¡®è®¤ä¸€ä¸‹ï¼Œä½ æ˜¯è¯´...'")
        recommendations.append("ğŸ” å»ºè®®ä½¿ç”¨'èƒ½å¦è¿›ä¸€æ­¥è§£é‡Š...'æ¥å¼•å¯¼æ·±åº¦è®¨è®º")
    elif engagement > 0.8:
        recommendations.append("ğŸ”¥ å½“å‰äº’åŠ¨éå¸¸æ´»è·ƒï¼Œå»ºè®®å¼•å¯¼å¯¹è¯æœå‘å…·ä½“è¡ŒåŠ¨è®¡åˆ’")
        recommendations.append("ğŸ“Š å¯ä»¥ä½¿ç”¨ç»“æ„åŒ–æ€»ç»“æ¥å·©å›ºè®¨è®ºæˆæœ")
    elif engagement > 0.6:
        recommendations.append("ğŸ’ª ä¼˜ç§€çš„å‚ä¸åº¦ï¼ç»§ç»­ä¿æŒè¿™ç§æ´»è·ƒçš„äº¤æµçŠ¶æ€")
        recommendations.append("ğŸŒŠ é€‚æ—¶åŠ å…¥è½¬æŠ˜æ€§å†…å®¹ï¼Œå¦‚'å¦ä¸€æ–¹é¢...'æ¥ä¸°å¯Œå¯¹è¯ç»´åº¦")
    
    # å¯¹è¯ç»“æ„ä¼˜åŒ–å»ºè®®
    if word_count < 50:
        recommendations.append("ğŸ’¬ å¯¹è¯è¾ƒä¸ºç®€çŸ­ï¼Œå»ºè®®æ·»åŠ èƒŒæ™¯ä¿¡æ¯æˆ–å…·ä½“ä¾‹å­æ¥å¢å¼ºè¯´æœåŠ›")
        recommendations.append("ğŸ“‹ å¯ä»¥ä½¿ç”¨'è®©æˆ‘è¯¦ç»†è¯´æ˜ä¸€ä¸‹...'æ¥å¢åŠ å†…å®¹çš„ä¸°å¯Œåº¦")
    elif word_count > 300:
        recommendations.append("ğŸ“š å¯¹è¯å†…å®¹ä¸°å¯Œï¼Œå»ºè®®åœ¨å…³é”®ç‚¹è¿›è¡Œæ€»ç»“å’Œç¡®è®¤")
        recommendations.append("ğŸ—‚ï¸ å¯ä»¥ä½¿ç”¨'æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰æåˆ°çš„...'æ¥æé«˜æ²Ÿé€šæ•ˆç‡")
    
    # é—®ç­”æ¨¡å¼çš„ä¼˜åŒ–å»ºè®®
    if question_count == 0:
        recommendations.append("â“ å»ºè®®å¢åŠ äº’åŠ¨å¼é—®é¢˜ï¼Œå¦‚'ä½ è§‰å¾—...å¦‚ä½•ï¼Ÿ'æ¥ä¿ƒè¿›å¯¹è¯")
        recommendations.append("ğŸ¤” å¯ä»¥æå‡ºæ¢ç´¢æ€§é—®é¢˜æ¥äº†è§£å¯¹æ–¹ç«‹åœºå’Œæƒ³æ³•")
    elif question_count > word_count / 10:
        recommendations.append("ğŸ—£ï¸ é—®é¢˜è¾ƒå¤šï¼Œå»ºè®®é€‚å½“æ·»åŠ æ›´å¤šé™ˆè¿°å’Œåˆ†äº«æ¥å¹³è¡¡å¯¹è¯")
        recommendations.append("ğŸ“Š å¯ä»¥åˆ†äº«ä¸ªäººè§è§£æˆ–ç»éªŒæ¥ä¸°å¯Œå¯¹è¯å†…å®¹")
    elif question_count < word_count / 50:
        recommendations.append("ğŸ’­ å¯ä»¥é€‚å½“å¢åŠ æ¾„æ¸…æ€§æˆ–ç¡®è®¤æ€§é—®é¢˜ï¼Œå¦‚'æˆ‘ç†è§£å¯¹å—ï¼Ÿ'")
    
    # åœºæ™¯ç‰¹å®šçš„æ·±åº¦å»ºè®®
    if scenario == "é¢è¯•":
        if sentiment_score > 0.3:
            recommendations.append("ğŸ¯ é¢è¯•è¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®ç»§ç»­å±•ç°ä¸“ä¸šèƒ½åŠ›å’Œä¸ªäººé­…åŠ›")
            recommendations.append("ğŸ’¼ å¯ä»¥é€‚å½“å±•ç¤ºå¯¹å…¬å¸å’ŒèŒä½çš„æ·±å…¥äº†è§£")
        else:
            recommendations.append("ğŸ“ é¢è¯•ç•¥æ˜¾ç´§å¼ ï¼Œå»ºè®®æ”¾æ…¢è¯­é€Ÿï¼Œå±•ç°æ›´å¤šè‡ªä¿¡")
            recommendations.append("ğŸ¨ å¯ä»¥é€šè¿‡å…·ä½“ä¾‹å­æ¥å±•ç¤ºè§£å†³é—®é¢˜çš„èƒ½åŠ›")
    elif scenario == "èŠå¤©":
        if engagement > 0.6:
            recommendations.append("ğŸ—£ï¸ èŠå¤©æ°›å›´å¾ˆå¥½ï¼Œå¯ä»¥æ·±å…¥åˆ†äº«æ›´å¤šä¸ªäººæ•…äº‹å’Œæ„Ÿå—")
            recommendations.append("ğŸŒˆ é€‚å½“åŠ å…¥å¹½é»˜å…ƒç´ æ¥å¢è¿›æƒ…æ„Ÿè¿æ¥")
        else:
            recommendations.append("ğŸ˜Œ èŠå¤©é£æ ¼æ¸©å’Œï¼Œå¯ä»¥å°è¯•æ‰¾åˆ°æ›´å¤šå…±åŒå…´è¶£ç‚¹")
            recommendations.append("ğŸ“¸ å¯ä»¥é€šè¿‡åˆ†äº«ç”Ÿæ´»ç»å†æ¥å¢åŠ å¯¹è¯çš„çœŸå®æ€§")
    elif scenario == "ä¼šè®®":
        recommendations.append("ğŸ¢ ä¿æŒä¸“ä¸šçš„ä¼šè®®æ²Ÿé€šé£æ ¼ï¼Œç¡®ä¿æ‰€æœ‰å‚ä¸è€…éƒ½æœ‰å‘è¨€æœºä¼š")
        recommendations.append("ğŸ“‹ å»ºè®®æ˜ç¡®è¡ŒåŠ¨é¡¹å’Œæ—¶é—´èŠ‚ç‚¹ï¼Œæé«˜ä¼šè®®æ•ˆç‡")
        recommendations.append("âœ… å¯ä»¥é€‚æ—¶æ€»ç»“è®¨è®ºæˆæœå¹¶ç¡®è®¤ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
    elif scenario == "å’¨è¯¢":
        recommendations.append("ğŸ§  ä¿æŒä¸“ä¸šçš„å’¨è¯¢æ€åº¦ï¼Œç¡®ä¿ä¸ºå¯¹æ–¹æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯")
        recommendations.append("ğŸ¯ å»ºè®®å…ˆç¡®è®¤å…·ä½“éœ€æ±‚ï¼Œå†æä¾›é’ˆå¯¹æ€§çš„å»ºè®®")
        recommendations.append("ğŸ“Š å¯ä»¥é€‚å½“å¼•ç”¨æ•°æ®æˆ–æ¡ˆä¾‹æ¥å¢å¼ºè¯´æœåŠ›")
    
    # æ²Ÿé€šæŠ€å·§ä¼˜åŒ–
    positive_words = ["è°¢è°¢", "æ„Ÿè°¢", "å¥½", "æ£’", "å–œæ¬¢", "æ»¡æ„", "åŒæ„", "æ”¯æŒ"]
    negative_words = ["ä¸å¥½", "è®¨åŒ", "å¤±æœ›", "ä¸æ»¡", "åå¯¹", "æ‹’ç»", "é—®é¢˜", "å›°éš¾"]
    
    positive_count = sum(1 for word in positive_words if word in dialogue)
    negative_count = sum(1 for word in negative_words if word in dialogue)
    
    if positive_count < 1:
        recommendations.append("ğŸ’ å»ºè®®å¢åŠ æ›´å¤šè¡¨è¾¾æ„Ÿè°¢å’Œè®¤å¯çš„è¯è¯­ï¼Œå¦‚'è°¢è°¢ä½ çš„åˆ†äº«'")
    
    if negative_count > positive_count * 1.5:
        recommendations.append("ğŸŒŸ å»ºè®®ä½¿ç”¨æ›´å¤šç§¯ææ­£å‘çš„è¡¨è¾¾æ¥æ”¹å–„å¯¹è¯æ°›å›´")
        recommendations.append("ğŸ”„ å¯ä»¥å°†é—®é¢˜å¯¼å‘çš„è¡¨è¿°è½¬æ¢ä¸ºè§£å†³æ–¹æ¡ˆå¯¼å‘")
    
    # ç¡®ä¿è‡³å°‘æœ‰3-5ä¸ªå»ºè®®
    if len(recommendations) < 3:
        recommendations.append("ğŸŒ± ç»§ç»­ç»ƒä¹ ä¸»åŠ¨å€¾å¬å’ŒåŒç†å¿ƒï¼Œè¿™å°†æ˜¾è‘—æå‡æ²Ÿé€šè´¨é‡")
        recommendations.append("ğŸ“– å¯ä»¥å­¦ä¹ ä¸€äº›æ²Ÿé€šæŠ€å·§ï¼Œå¦‚ä½¿ç”¨'æˆ‘ç†è§£ä½ çš„è§‚ç‚¹...'ç­‰è¡¨è¾¾")
    
    return recommendations[:5]  # é™åˆ¶åœ¨5ä¸ªå»ºè®®ä»¥å†…ï¼Œé¿å…ä¿¡æ¯è¿‡è½½


def _generate_pulse_points(word_count: int, sentiment_score: float, engagement: float) -> list:
    """ç”Ÿæˆè„‰å†²ç‚¹"""
    pulse_points = []
    
    # æ ¹æ®å¯¹è¯é•¿åº¦ç”Ÿæˆè„‰å†²ç‚¹æ•°é‡
    num_points = min(5, max(2, word_count // 50))
    
    import time
    base_timestamp = int(time.time())
    
    for i in range(num_points):
        # åŸºç¡€æ—¶é—´æˆ³ï¼ˆæ¯30ç§’ä¸€ä¸ªç‚¹ï¼‰
        timestamp = base_timestamp - (num_points - i - 1) * 30
        
        # ç”ŸæˆåŠ¨æ€çš„æŒ‡æ ‡å€¼
        time_factor = i / max(1, num_points - 1)  # 0åˆ°1çš„æ—¶é—´å› å­
        
        # å¼ºåº¦éšæ—¶é—´æœ‰æ³¢åŠ¨
        intensity = round(0.3 + 0.4 * engagement + 0.3 * (0.5 + 0.5 * sentiment_score) + 0.2 * (0.5 + 0.5 * (-1) ** i), 2)
        intensity = max(0.1, min(1.0, intensity))
        
        # æƒ…æ„ŸåŸºäºæ•´ä½“æƒ…æ„Ÿå€¾å‘å¹¶æœ‰æ³¢åŠ¨
        sentiment = "positive" if sentiment_score > 0.1 else "negative" if sentiment_score < -0.1 else "neutral"
        
        # å‚ä¸åº¦åŸºäºæ€»ä½“å‚ä¸åº¦å¹¶æœ‰æ—¶é—´å˜åŒ–
        engagement_point = round(max(0.1, min(1.0, engagement + 0.2 * (0.5 - abs(time_factor - 0.5)))), 2)
        
        # æ¸…æ™°åº¦åŸºäºæƒ…æ„Ÿç¨³å®šæ€§å’Œå‚ä¸åº¦
        clarity = round(0.4 + 0.3 * (1 - abs(sentiment_score)) + 0.3 * engagement_point, 2)
        
        pulse_points.append({
            "timestamp": f"2025-01-15T{10 + i // 2:02d}:{(i % 2) * 30:02d}:00Z",
            "intensity": intensity,
            "sentiment": sentiment,
            "engagement": engagement_point,
            "clarity": clarity,
            "speaker_role": f"participant_{chr(65 + i % 2)}"  # participant_A, participant_B
        })
    
    return pulse_points

@api_router.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    return {
        "status": "healthy",
        "service": "LingoPulse Backend",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }


@api_router.get("/stats")
async def get_service_stats():
    """
    è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
    """
    # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»ç›‘æ§ç³»ç»Ÿä¸­è·å–
    return {
        "total_conversations": 0,
        "total_analyses": 0,
        "active_analyses": 0,
        "average_analysis_time": 0,
        "uptime": "0h 0m 0s",
        "last_updated": datetime.now().isoformat()
    }


# åå°ä»»åŠ¡å‡½æ•°
async def execute_batch_analysis(conversation_ids: List[str], max_concurrent: int):
    """
    æ‰§è¡Œæ‰¹é‡åˆ†æçš„åå°ä»»åŠ¡
    """
    try:
        # è¿™é‡Œéœ€è¦ä»åº”ç”¨å±‚è·å–ç”¨ä¾‹å®ä¾‹
        # results = await batch_analyze_use_case.execute(conversation_ids, max_concurrent)
        print(f"Batch analysis started for {len(conversation_ids)} conversations")
        # å¤„ç†åˆ†æç»“æœ...
    except Exception as e:
        print(f"Batch analysis failed: {e}")


# å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥ç›¸å…³æ¨¡å‹
class WeChatUploadRequest(BaseModel):
    """å¾®ä¿¡èŠå¤©è®°å½•ä¸Šä¼ è¯·æ±‚"""
    conversation_name: str = Field(..., description="å¯¹è¯åç§°")
    participants: List[str] = Field(..., description="å‚ä¸è€…åˆ—è¡¨")
    conversation_type: str = Field(default="wechat", description="å¯¹è¯ç±»å‹")


class WeChatAnalysisRequest(BaseModel):
    """å¾®ä¿¡èŠå¤©è®°å½•åˆ†æè¯·æ±‚"""
    conversation_id: str = Field(..., description="å¯¹è¯ID")


class WeChatBatchImportRequest(BaseModel):
    """å¾®ä¿¡èŠå¤©è®°å½•æ‰¹é‡å¯¼å…¥è¯·æ±‚"""
    file_paths: List[str] = Field(..., description="æ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    conversation_name: str = Field(..., description="é»˜è®¤å¯¹è¯åç§°")
    participants: List[str] = Field(..., description="å‚ä¸è€…åˆ—è¡¨")


# å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥ç›¸å…³ç«¯ç‚¹
@api_router.post("/wechat/upload", response_model=StatusResponse)
async def upload_wechat_file(
    file: UploadFile = File(...),
    request: WeChatUploadRequest = Depends(),
):
    """
    ä¸Šä¼ å¾®ä¿¡èŠå¤©è®°å½•æ–‡ä»¶
    """
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.txt', '.json', '.csv'}
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}"
            )
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        uploads_dir = Path("uploads/wechat")
        uploads_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = uploads_dir / f"{uuid.uuid4()}_{file.filename}"
        
        with open(file_path, 'wb') as buffer:
            content = await file.read()
            buffer.write(content)
        
        # åˆå§‹åŒ–å¾®ä¿¡å¯¼å…¥å™¨
        importer = WeChatChatImporter()
        
        # åœ¨åå°ä»»åŠ¡ä¸­å¤„ç†æ–‡ä»¶å¯¼å…¥
        background_tasks.add_task(
            process_wechat_import,
            str(file_path),
            request.conversation_name,
            request.participants,
            request.conversation_type
        )
        
        return StatusResponse(
            status="uploaded",
            message="å¾®ä¿¡èŠå¤©è®°å½•æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­...",
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")


@api_router.post("/wechat/import", response_model=Dict[str, Any])
async def import_wechat_chat_record(
    file_path: str,
    conversation_name: str,
    participants: List[str],
    conversation_type: str = "wechat"
):
    """
    å¯¼å…¥å¾®ä¿¡èŠå¤©è®°å½•
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file_path).exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åˆå§‹åŒ–å¾®ä¿¡å¯¼å…¥å™¨
        importer = WeChatChatImporter()
        
        # è§£æèŠå¤©è®°å½•
        conversations = importer.import_chat_record(file_path)
        
        # éªŒè¯å’Œæ ‡å‡†åŒ–æ•°æ®
        validated_conversations = []
        
        for conv_data in conversations:
            # éªŒè¯å¿…å¡«å­—æ®µ
            if not conv_data.get('messages'):
                continue
                
            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            validated_conv = {
                'id': str(uuid.uuid4()),
                'title': conversation_name or f"å¾®ä¿¡å¯¹è¯_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'participants': participants or ['æœªçŸ¥å‚ä¸è€…'],
                'conversation_type': conversation_type,
                'messages': conv_data['messages'],
                'created_at': datetime.now(),
                'metadata': {
                    'source': 'wechat',
                    'original_file': file_path,
                    'import_time': datetime.now().isoformat()
                }
            }
            validated_conversations.append(validated_conv)
        
        return {
            "status": "success",
            "message": f"æˆåŠŸå¯¼å…¥ {len(validated_conversations)} ä¸ªå¯¹è¯",
            "conversations": validated_conversations,
            "total_messages": sum(len(conv['messages']) for conv in validated_conversations),
            "participants": participants,
            "import_time": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {str(e)}")


@api_router.get("/wechat/analysis/{conversation_id}")
async def analyze_wechat_conversation(conversation_id: str):
    """
    åˆ†æå¾®ä¿¡èŠå¤©è®°å½•
    """
    try:
        # è¿™é‡Œéœ€è¦æ ¹æ®conversation_idè·å–å¯¹è¯æ•°æ®
        # ç„¶åè°ƒç”¨åˆ†æåŠŸèƒ½
        # ç®€åŒ–å®ç°ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        
        analysis_result = {
            "conversation_id": conversation_id,
            "analysis_type": "wechat_chat_analysis",
            "results": {
                "communication_patterns": {
                    "message_frequency": "ä¸­ç­‰",
                    "response_time_avg": "2.5åˆ†é’Ÿ",
                    "active_hours": ["09:00-12:00", "14:00-18:00", "20:00-22:00"]
                },
                "sentiment_analysis": {
                    "overall_sentiment": "ç§¯æ",
                    "sentiment_trend": "ç¨³å®š",
                    "emotional_peaks": 3
                },
                "interaction_quality": {
                    "engagement_score": 0.85,
                    "clarity_score": 0.78,
                    "collaboration_score": 0.82
                },
                "key_topics": ["å·¥ä½œè®¨è®º", "æ—¥å¸¸é—²èŠ", "é—®é¢˜è§£å†³"],
                "communication_style": "å‹å¥½ä¸”ä¸“ä¸š"
            },
            "recommendations": [
                "ä¿æŒå½“å‰çš„ç§¯ææ²Ÿé€šé£æ ¼",
                "åœ¨å·¥ä½œæ—¶é—´å†…ä¿æŒå“åº”é€Ÿåº¦",
                "å¢åŠ æ›´å¤šå»ºè®¾æ€§çš„è®¨è®ºè¯é¢˜"
            ],
            "analyzed_at": datetime.now().isoformat()
        }
        
        return analysis_result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@api_router.post("/wechat/batch-import")
async def batch_import_wechat_records(
    background_tasks: BackgroundTasks,
    request: WeChatBatchImportRequest = Depends(),
):
    """
    æ‰¹é‡å¯¼å…¥å¾®ä¿¡èŠå¤©è®°å½•
    """
    try:
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        valid_files = []
        for file_path in request.file_paths:
            if Path(file_path).exists():
                valid_files.append(file_path)
            else:
                logging.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not valid_files:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶")
        
        # å¯åŠ¨åå°æ‰¹é‡å¯¼å…¥ä»»åŠ¡
        background_tasks.add_task(
            process_batch_wechat_import,
            valid_files,
            request.conversation_name,
            request.participants
        )
        
        return StatusResponse(
            status="accepted",
            message=f"æ‰¹é‡å¯¼å…¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œå…± {len(valid_files)} ä¸ªæ–‡ä»¶",
            timestamp=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {str(e)}")


@api_router.get("/wechat/formats")
async def get_supported_wechat_formats():
    """
    è·å–æ”¯æŒçš„å¾®ä¿¡èŠå¤©è®°å½•æ ¼å¼
    """
    return {
        "supported_formats": [
            {
                "extension": ".txt",
                "description": "å¾®ä¿¡å¯¼å‡ºçš„æ–‡æœ¬æ ¼å¼èŠå¤©è®°å½•",
                "example_structure": "2024-01-01 12:00:00 å¼ ä¸‰: ä½ å¥½",
                "required_fields": ["timestamp", "sender", "content"]
            },
            {
                "extension": ".json",
                "description": "ç»“æ„åŒ–çš„JSONæ ¼å¼èŠå¤©è®°å½•",
                "example_structure": {
                    "messages": [
                        {"timestamp": "2024-01-01T12:00:00", "sender": "å¼ ä¸‰", "content": "ä½ å¥½"}
                    ]
                },
                "required_fields": ["messages"]
            },
            {
                "extension": ".csv",
                "description": "é€—å·åˆ†éš”çš„CSVæ ¼å¼èŠå¤©è®°å½•",
                "example_structure": "timestamp,sender,content\\n2024-01-01 12:00:00,å¼ ä¸‰,ä½ å¥½",
                "required_fields": ["timestamp", "sender", "content"]
            }
        ],
        "max_file_size": "50MB",
        "max_messages_per_file": 10000,
        "supported_encodings": ["UTF-8", "GBK", "GB2312"]
    }


# åå°ä»»åŠ¡å‡½æ•°
async def process_wechat_import(
    file_path: str,
    conversation_name: str,
    participants: List[str],
    conversation_type: str
):
    """
    å¤„ç†å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥çš„åå°ä»»åŠ¡
    """
    try:
        logging.info(f"å¼€å§‹å¤„ç†å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥: {file_path}")
        
        # åˆå§‹åŒ–å¾®ä¿¡å¯¼å…¥å™¨
        importer = WeChatChatImporter()
        
        # è§£æèŠå¤©è®°å½•
        conversations = importer.import_chat_record(file_path)
        
        # è¿™é‡Œåº”è¯¥å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
        # ç®€åŒ–å®ç°ï¼Œåªè®°å½•æ—¥å¿—
        logging.info(f"æˆåŠŸå¯¼å…¥ {len(conversations)} ä¸ªå¯¹è¯ï¼Œå…± {sum(len(conv.get('messages', [])) for conv in conversations)} æ¡æ¶ˆæ¯")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if Path(file_path).exists():
            Path(file_path).unlink()
            logging.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            
    except Exception as e:
        logging.error(f"å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥å¤±è´¥: {e}", exc_info=True)


async def process_batch_wechat_import(
    file_paths: List[str],
    conversation_name: str,
    participants: List[str]
):
    """
    å¤„ç†æ‰¹é‡å¾®ä¿¡èŠå¤©è®°å½•å¯¼å…¥çš„åå°ä»»åŠ¡
    """
    try:
        logging.info(f"å¼€å§‹æ‰¹é‡å¯¼å…¥å¾®ä¿¡èŠå¤©è®°å½•ï¼Œå…± {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        importer = WeChatChatImporter()
        total_conversations = 0
        total_messages = 0
        
        for file_path in file_paths:
            try:
                conversations = importer.import_chat_record(file_path)
                total_conversations += len(conversations)
                total_messages += sum(len(conv.get('messages', [])) for conv in conversations)
                
                logging.info(f"å¤„ç†æ–‡ä»¶ {file_path}: {len(conversations)} ä¸ªå¯¹è¯")
                
            except Exception as e:
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        logging.info(f"æ‰¹é‡å¯¼å…¥å®Œæˆ: {total_conversations} ä¸ªå¯¹è¯ï¼Œ{total_messages} æ¡æ¶ˆæ¯")
        
    except Exception as e:
        logging.error(f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}", exc_info=True)


# æ³¨å†Œå¾®ä¿¡æå–å™¨è·¯ç”±
api_router.include_router(wechat_extractor_router)